{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordSenteceEmbeddingBrrtCUB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgfCpa4DkYZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9ecda4b3-2e55-479c-ddeb-dd8032aa2284"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YzYHfuZkyBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl-Q24tmk97o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d437d8bb-30ca-4d56-9c9b-36fbc323fb1e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/CUB_captions.csv\", header=None, names=['Label',  'ID', 'Sentences'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,856\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ssnIAMelFlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "import numpy as np\n",
        "\n",
        "sentences = df.Sentences.values\n",
        "labels_text = df.Label.values\n",
        "labels = df.ID.values\n",
        "\n",
        "labels = np.delete(labels, 0)\n",
        "labels_text = np.delete(labels_text, 0)\n",
        "sentences = np.delete(sentences, 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5HCLYT_lQNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer , BertModel\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5go8gffQlTqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    if  len(input_ids) == 80:\n",
        "      print('sent with max len :', sent)\n",
        "      print('number of words insent with max len :', len(sent.split()))\n",
        "      print('index of sent with max len :', np.where(sentences==sent))\n",
        "\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "atwal = []\n",
        "for i in sentences:\n",
        "  atwal.append(len(i.split()))\n",
        "atwal.sort(reverse=True)\n",
        "print('Max number of words in a sentnces length: ', atwal[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5hSFfelaJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation=True\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = labels.astype(int)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "segments_ids = []\n",
        "for i in range(input_ids.size()[0]):\n",
        "  segments_id = [1] * input_ids.size()[1]\n",
        "  segments_ids.append(segments_id)\n",
        "\n",
        "segments_ids = torch.tensor(segments_ids)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8jSw87rlw4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, segments_ids, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(1 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset = dataset\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4glZsFCmG4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wq68VwymUFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db6qDBiXmYdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "7860b23a-58ea-4a28-cb5e-988fbfacf492"
      },
      "source": [
        "import numpy as np\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  if step % 40 == 0 :  \n",
        "    # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "    # `to` method.\n",
        "    #\n",
        "    # `batch` contains three pytorch tensors:\n",
        "    #   [0]: input ids \n",
        "    #   [1]: attention masks\n",
        "    #   [2]: labels \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_segments_ids = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    print(device,'=> ', step)\n",
        "    #print(b_segments_ids.size())\n",
        "    #print(b_segments_ids[0])\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "          outputs = model(b_input_ids, b_segments_ids)\n",
        "          hidden_states = outputs[2]\n",
        "    \n",
        "\n",
        "    # Concatenate the tensors for all layers. We use `stack` here to\n",
        "    # create a new dimension in the tensor.\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "    # Swap dimensions 0 and 1.\n",
        "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
        "\n",
        "    word_vecs_sum = []\n",
        "    for sentence in token_embeddings:\n",
        "      # `sentence` is a [64 x 12 x 768] tensor.\n",
        "      \n",
        "      # Stores the token vectors, with shape [64 x 768]\n",
        "      token_vecs_sum = []\n",
        "      # For each token in the sentence...\n",
        "      \n",
        "      for token in sentence:\n",
        "        # `token` is a [12 x 768] tensor\n",
        "        # Sum the vectors from the last four layers.\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "        # Use `sum_vec` to represent `token`.\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "\n",
        "      # Use `token_vecs_sum` to represent `sentence`.\n",
        "      word_vecs_sum.append(token_vecs_sum)\n",
        "\n",
        "    print ('Word Embedding Shape is: %d x %d x %d' % (len(word_vecs_sum), len(word_vecs_sum[0]), len(word_vecs_sum[0][0])))\n",
        "\n",
        "\n",
        "    # `hidden_states` has shape [13 x 32 x 64 x 768]\n",
        "    # `token_vecs` is a tensor with shape [64 x 768]\n",
        "    token_vecs = hidden_states[-2]\n",
        "\n",
        "    # Calculate the average of all 64 token vectors.\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=1)\n",
        "    print('Sentence Embedding Shape is: ', sentence_embedding.size())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda =>  0\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n",
            "cuda =>  40\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n",
            "cuda =>  80\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n",
            "cuda =>  120\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n",
            "cuda =>  160\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n",
            "cuda =>  200\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n",
            "cuda =>  240\n",
            "Word Embedding Shape is: 32 x 64 x 768\n",
            "Sentence Embedding Shape is:  torch.Size([32, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-XTfvTHnvlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}