{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "checkGPU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23cc828aada340529db52f45f0ba455e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de412740f1ef4a15863ea307adb1f47d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3be427f74f14dbb905696ffc0a71b16",
              "IPY_MODEL_caf971d180dd4018b11b9275117c42c4"
            ]
          }
        },
        "de412740f1ef4a15863ea307adb1f47d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3be427f74f14dbb905696ffc0a71b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1c29f87fab8445faa540db109d5951a1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e5180776148445594165c7f4d90fa8a"
          }
        },
        "caf971d180dd4018b11b9275117c42c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd9336cb3cfe4e22a69f2e215775cc44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 896kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_466b6accea9341099841be6098e06a95"
          }
        },
        "1c29f87fab8445faa540db109d5951a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e5180776148445594165c7f4d90fa8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd9336cb3cfe4e22a69f2e215775cc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "466b6accea9341099841be6098e06a95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mVHKOFJKObV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d297536d-09fd-4692-fd24-ca48f7f86d42"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 16 11:25:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X39EBfvPKZkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786,
          "referenced_widgets": [
            "23cc828aada340529db52f45f0ba455e",
            "de412740f1ef4a15863ea307adb1f47d",
            "b3be427f74f14dbb905696ffc0a71b16",
            "caf971d180dd4018b11b9275117c42c4",
            "1c29f87fab8445faa540db109d5951a1",
            "7e5180776148445594165c7f4d90fa8a",
            "bd9336cb3cfe4e22a69f2e215775cc44",
            "466b6accea9341099841be6098e06a95"
          ]
        },
        "outputId": "b8d7571e-9d5c-4341-df17-cd0873d0e362"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer , BertModel\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():        \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/only_captions.csv\", header=None)\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "sentences = df[1].values\n",
        "sentences = np.delete(sentences, 0)\n",
        "\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens = True, max_length = 18, pad_to_max_length = True, return_attention_mask = True,   \n",
        "                        return_tensors = 'pt', truncation=True)\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "segments_ids = []\n",
        "for i in range(input_ids.size()[0]):\n",
        "  segments_id = [1] * input_ids.size()[1]\n",
        "  segments_ids.append(segments_id)\n",
        "segments_ids = torch.tensor(segments_ids)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 5.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 29.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 57.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 59.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d93311cc4cf714c1b3cd44b211e0bdb1d617977487ebb69f1c3ae48a11b7d642\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Number of training sentences: 88,551\n",
            "\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23cc828aada340529db52f45f0ba455e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original:  a bird with a very long wing span and a long pointed beak.\n",
            "Token IDs: tensor([  101,  1037,  4743,  2007,  1037,  2200,  2146,  3358,  8487,  1998,\n",
            "         1037,  2146,  4197, 23525,  1012,   102,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_FHQhAj9Fy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "b5c26a92-c95e-4bbb-cb90-e186171a7ec4"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "!rm -r sample_data\n",
        "\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\" -O CUB_200_2011.tgz && rm -rf /tmp/cookies.txt\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AttnGAN'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 36.76 MiB | 38.25 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "--2020-07-16 11:34:56--  https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.101, 74.125.195.139, 74.125.195.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ma8agsk7m73702qgc9utn0j1uoi6e0c1/1594899300000/09657060183789739732/*/1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-16 11:35:25--  https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ma8agsk7m73702qgc9utn0j1uoi6e0c1/1594899300000/09657060183789739732/*/1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ?e=download\n",
            "Resolving doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)... 74.125.28.132, 2607:f8b0:400e:c04::84\n",
            "Connecting to doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)|74.125.28.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: â€˜birds.zipâ€™\n",
            "\n",
            "birds.zip               [ <=>                ]   6.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-07-16 11:35:26 (57.8 MB/s) - â€˜birds.zipâ€™ saved [6488322]\n",
            "\n",
            "--2020-07-16 11:35:39--  https://docs.google.com/uc?export=download&confirm=xBzM&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.138, 74.125.195.113, 74.125.195.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-68-docs.googleusercontent.com/docs/securesc/4vgtau7fsa7qtilu9ctsjbb827v2d4p8/8gbk2k48p75h4dk3559cjolcrunms2su/1594899300000/15424859768005087218/10939505658550217153Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download [following]\n",
            "--2020-07-16 11:35:39--  https://doc-0c-68-docs.googleusercontent.com/docs/securesc/4vgtau7fsa7qtilu9ctsjbb827v2d4p8/8gbk2k48p75h4dk3559cjolcrunms2su/1594899300000/15424859768005087218/10939505658550217153Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download\n",
            "Resolving doc-0c-68-docs.googleusercontent.com (doc-0c-68-docs.googleusercontent.com)... 74.125.28.132, 2607:f8b0:400e:c04::84\n",
            "Connecting to doc-0c-68-docs.googleusercontent.com (doc-0c-68-docs.googleusercontent.com)|74.125.28.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=tv5hnfojuipn6&continue=https://doc-0c-68-docs.googleusercontent.com/docs/securesc/4vgtau7fsa7qtilu9ctsjbb827v2d4p8/8gbk2k48p75h4dk3559cjolcrunms2su/1594899300000/15424859768005087218/10939505658550217153Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e%3Ddownload&hash=nhs4mh635p9fdtqnthvckbgp2s0l8rls [following]\n",
            "--2020-07-16 11:35:40--  https://docs.google.com/nonceSigner?nonce=tv5hnfojuipn6&continue=https://doc-0c-68-docs.googleusercontent.com/docs/securesc/4vgtau7fsa7qtilu9ctsjbb827v2d4p8/8gbk2k48p75h4dk3559cjolcrunms2su/1594899300000/15424859768005087218/10939505658550217153Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e%3Ddownload&hash=nhs4mh635p9fdtqnthvckbgp2s0l8rls\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0c-68-docs.googleusercontent.com/docs/securesc/4vgtau7fsa7qtilu9ctsjbb827v2d4p8/8gbk2k48p75h4dk3559cjolcrunms2su/1594899300000/15424859768005087218/10939505658550217153Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download&nonce=tv5hnfojuipn6&user=10939505658550217153Z&hash=mefl1sippeqht123g3o4bb4blqivfot3 [following]\n",
            "--2020-07-16 11:35:40--  https://doc-0c-68-docs.googleusercontent.com/docs/securesc/4vgtau7fsa7qtilu9ctsjbb827v2d4p8/8gbk2k48p75h4dk3559cjolcrunms2su/1594899300000/15424859768005087218/10939505658550217153Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download&nonce=tv5hnfojuipn6&user=10939505658550217153Z&hash=mefl1sippeqht123g3o4bb4blqivfot3\n",
            "Connecting to doc-0c-68-docs.googleusercontent.com (doc-0c-68-docs.googleusercontent.com)|74.125.28.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gtar]\n",
            "Saving to: â€˜CUB_200_2011.tgzâ€™\n",
            "\n",
            "CUB_200_2011.tgz        [   <=>              ]   1009M  88.3MB/s               "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JortsGgh-YVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from copy import deepcopy\n",
        "import skimage.transform\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None, transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys, input_ids, segments_ids = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    input_ids = input_ids.cuda()\n",
        "    segments_ids = segments_ids.cuda()\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys, input_ids, segments_ids]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    \n",
        "    COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
        "                2:[70, 70, 70],  3:[102,102,156],\n",
        "                4:[190,153,153], 5:[153,153,153],\n",
        "                6:[250,170, 30], 7:[220, 220, 0],\n",
        "                8:[107,142, 35], 9:[152,251,152],\n",
        "                10:[70,130,180], 11:[220,20, 60],\n",
        "                12:[255, 0, 0],  13:[0, 0, 142],\n",
        "                14:[119,11, 32], 15:[0, 60,100],\n",
        "                16:[0, 80, 100], 17:[0, 0, 230],\n",
        "                18:[0,  0, 70],  19:[0, 0,  0]}\n",
        "    FONT_MAX = 50\n",
        "\n",
        "    \n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None, input_ids=None, segments_ids=None, sentences=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []# [299]\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "        print(\"self.imsize\", self.imsize)\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox() # 11788 long dictionry with key as image name and value is 4 ints list bounding box\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "        #filenames: List of 8855 text items of image names\n",
        "        #captions: List of 8855 varible lengths captions -in range 9-18 -\n",
        "        #ixtoword: dictionry  of 5450 index [key] to word [value] pairs\n",
        "        #wordtoix: dictionry  of 5450 word [key] to index [value] pairs\n",
        "        #n_words: 5450\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames)) #200 classes, len:8855\n",
        "\n",
        "        self.number_example = len(self.filenames) #8855\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.segments_ids = segments_ids\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'% (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "\n",
        "        caps_dec = []\n",
        "        for word in caps:\n",
        "            caps_dec.append(self.ixtoword[int(word)])\n",
        "\n",
        "        return imgs, caps, cap_len, cls_id, key, self.input_ids[index], self.segments_ids[index]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "        \n",
        "        print('CNN_ENCODER')\n",
        "        model = models.inception_v3()\n",
        "        print('loaded_inception')\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "class BERT_ENCODER(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT_ENCODER, self).__init__()\n",
        "\n",
        "        \n",
        "        print('Bert_ENCODER')\n",
        "        \n",
        "        model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from  BertModel')\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "        self.bert_model = model\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.word_bert_code = nn.Linear(768, 256)\n",
        "        self.sent_bert_code = nn.Linear(768, 256)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.word_bert_code.weight.data.uniform_(-initrange, initrange)\n",
        "        self.sent_bert_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self,  b_input_ids, b_segments_ids):\n",
        "        \n",
        "        outputs = self.bert_model(b_input_ids, b_segments_ids)\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        word_embedding = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "\n",
        "        word_embedding= self.word_bert_code(word_embedding)\n",
        "\n",
        "\n",
        "        word_embedding = word_embedding.permute(1,3,0,2)\n",
        "\n",
        "        word_embedding = word_embedding.sum(dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        token_vecs = hidden_states[-2]\n",
        "\n",
        "        # Calculate the average of all 18 token vectors.\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=1)\n",
        "        sentence_embedding= self.sent_bert_code(sentence_embedding)\n",
        "\n",
        "\n",
        "        return  word_embedding, sentence_embedding\n",
        "\n",
        "\n",
        "\n",
        "def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
        "    \n",
        "    FONT_MAX = 50\n",
        "\n",
        "    num = captions.size(0)\n",
        "    img_txt = Image.fromarray(convas)\n",
        "    # get a font\n",
        "    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    print (\"CURRENT WORKING DIRCTORY : \" , os.getcwd())\n",
        "    fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    # get a drawing context\n",
        "    d = ImageDraw.Draw(img_txt)\n",
        "    sentence_list = []\n",
        "    for i in range(num):\n",
        "        cap = captions[i].data.cpu().numpy()\n",
        "        sentence = []\n",
        "        for j in range(len(cap)):\n",
        "            if cap[j] == 0:\n",
        "                break\n",
        "            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
        "            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
        "                   font=fnt, fill=(255, 255, 255, 255))\n",
        "            sentence.append(word)\n",
        "        sentence_list.append(sentence)\n",
        "    return img_txt, sentence_list\n",
        "\n",
        "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    w12 = torch.sum(x1 * x2, dim)\n",
        "    w1 = torch.norm(x1, 2, dim)\n",
        "    w2 = torch.norm(x2, 2, dim)\n",
        "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids, batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels, cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "    \n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, bert_encoder, batch_size, labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train() #Sets the module in training mode.\n",
        "    #rnn_model.train() #Sets the module in training mode.\n",
        "    bert_encoder.train()\n",
        "    \n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "    \n",
        "    \n",
        "        # Loading the first batch (number of batches/steps in an epoch is 183)\n",
        "        #rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, class_ids, keys, b_input_ids, b_segments_ids = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x 256 x 17 x 17 ==> # image region features\n",
        "        # sent_code: batch_size x 256                ==> # global image features\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)# 256, 17(16th of the whole image)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        #hidden = rnn_model.init_hidden(batch_size) # A tuple of 2 zero tensor of torch.Size([2, 48, 128])\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        #words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        #-------------------------------------------------------------------------------\n",
        "                #---------------------------------------------------------\n",
        "                        #-----------------------------------\n",
        "        words_emb, sent_emb =bert_encoder(b_input_ids, b_segments_ids)\n",
        "                        #-----------------------------------\n",
        "                #---------------------------------------------------------\n",
        "        #-------------------------------------------------------------------------------\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels, cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(bert_encoder.parameters(), cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 20 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = build_super_images(imgs[-1].cpu(), captions, ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/BertCUBImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, bert_encoder, batch_size):\n",
        "    cnn_model.eval()\n",
        "    bert_encoder.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, class_ids, keys, b_input_ids, b_segments_ids = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "        words_emb, sent_emb =bert_encoder(b_input_ids, b_segments_ids)\n",
        "\n",
        "\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels, cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    #text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    '''\n",
        "    RNN_ENCODER(\n",
        "    (encoder): Embedding(5450, 300)\n",
        "    (drop): Dropout(p=0.5, inplace=False)\n",
        "    (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True))\n",
        "    '''\n",
        "    print('build_models')\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    bert_encoder = BERT_ENCODER()\n",
        "\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    '''\n",
        "    A tensor of [0,1,2,3,...,47]\n",
        "    '''\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        bert_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        #text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "        bert_encoder = bert_encoder.cuda()\n",
        "\n",
        "    return bert_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    UPDATE_INTERVAL = 200\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = 299\n",
        "    batch_size = 48\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(355), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform, input_ids=input_ids, segments_ids=segments_ids, sentences=sentences )\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "    #using prepare data functiont this dataloader yieldes:\n",
        "    #imgs: a list of 1 tensor of size torch.Size([48, 3, 299, 299])\n",
        "    #captons: a  tensor of size torch.Size([48, 18]), shorter filled with end words converted by word to index\n",
        "    #cap_lens: a  tensor of size torch.Size([48]) , acual caps lens order from big to small (max is 18)\n",
        "    #class_ids: a 48 ints list in range 0-200 of the classes\n",
        "    #keys: a 48 string list  of the classes classes nammes crosspondening to the class_ids\n",
        "    \n",
        "\n",
        "    # # validation data #\n",
        "    dataset_val = TextDataset(cfg.DATA_DIR, 'test', base_size=cfg.TREE.BASE_SIZE,transform=image_transform, input_ids=input_ids, segments_ids=segments_ids, sentences=sentences)\n",
        "    print(dataset_val.n_words, dataset_val.embeddings_num)\n",
        "    assert dataset_val\n",
        "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, drop_last=True,shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    bert_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = []\n",
        "    \n",
        "    for v in bert_encoder.parameters(): # 4 parameters\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    \n",
        "    for v in image_encoder.parameters(): # 3 parameters\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "\n",
        "    print ('requires_grad =' , len(para))\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR #0.002\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            \n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, bert_encoder, batch_size, labels, optimizer, epoch, dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder, bert_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss ''{:5.2f} {:5.2f} | lr {:.5f}|'.format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 20 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                mydrivemodel = '/content/drive/My Drive/BertCUBModel'\n",
        "                torch.save(image_encoder.state_dict(), '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(image_encoder.state_dict(), '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(bert_encoder.state_dict(), '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(bert_encoder.state_dict(), '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}