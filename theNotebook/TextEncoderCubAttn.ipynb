{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextEncoderCubAttn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa03775b29974417b41fbd64953dfe8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4ad38c15dadd4b2a90e4d8761121b272",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d50187b34a447278202d1b6ea86ceb2",
              "IPY_MODEL_88792eb2b0354713923855af0f200bff"
            ]
          }
        },
        "4ad38c15dadd4b2a90e4d8761121b272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d50187b34a447278202d1b6ea86ceb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_28ebfd706ae34cd7a4d3809e3ce07e1a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b85a14c40404f5ba263eec7b18b39c5"
          }
        },
        "88792eb2b0354713923855af0f200bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f1df89e12ec64d90aeb4e393a9b6b1c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 653kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7071ad2f1954960922567dea36308fa"
          }
        },
        "28ebfd706ae34cd7a4d3809e3ce07e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b85a14c40404f5ba263eec7b18b39c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1df89e12ec64d90aeb4e393a9b6b1c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7071ad2f1954960922567dea36308fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SJ2aHTi7QUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data\n",
        "\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\" -O CUB_200_2011.tgz && rm -rf /tmp/cookies.txt\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gONfh1Fi2eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from copy import deepcopy\n",
        "import skimage.transform\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    \n",
        "    COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
        "                2:[70, 70, 70],  3:[102,102,156],\n",
        "                4:[190,153,153], 5:[153,153,153],\n",
        "                6:[250,170, 30], 7:[220, 220, 0],\n",
        "                8:[107,142, 35], 9:[152,251,152],\n",
        "                10:[70,130,180], 11:[220,20, 60],\n",
        "                12:[255, 0, 0],  13:[0, 0, 142],\n",
        "                14:[119,11, 32], 15:[0, 60,100],\n",
        "                16:[0, 80, 100], 17:[0, 0, 230],\n",
        "                18:[0,  0, 70],  19:[0, 0,  0]}\n",
        "    FONT_MAX = 50\n",
        "\n",
        "    \n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []# [299]\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "        print(\"self.imsize\", self.imsize)\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox() # 11788 long dictionry with key as image name and value is 4 ints list bounding box\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "        #filenames: List of 8855 text items of image names\n",
        "        #captions: List of 885 varible lengths captions -in range 9-18 -\n",
        "        #ixtoword: dictionry  of 5450 index [key] to word [value] pairs\n",
        "        #wordtoix: dictionry  of 5450 word [key] to index [value] pairs\n",
        "        #n_words: 5450\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames)) #200 classes, len:8855\n",
        "\n",
        "        self.number_example = len(self.filenames) #8855\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                    nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM # max length and padded to captions= 18\n",
        "        self.ntoken = ntoken  # size of the dictionary = 5450\n",
        "        self.ninput = ninput  # size of each embedding vector = 300\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed = 0.5\n",
        "        self.nlayers = nlayers  # Number of recurrent layers =1\n",
        "        self.bidirectional = bidirectional # True\n",
        "        self.rnn_type = cfg.RNN_TYPE #LSTM\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions # 128\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                                self.nlayers, batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                                self.nlayers, batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions, bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        #emb[0]: a tensor of torch.Size([660, 300])\n",
        "        #emb[1]: a tensor of torch.Size([18])\n",
        "        #emb[2]: None\n",
        "        #emb[3]: None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "\n",
        "\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        #output[0]: a tensor of torch.Size([660, 256])\n",
        "        #output[1]: a tensor of torch.Size([18])\n",
        "        #output[2]: None\n",
        "        #output[3]: None\n",
        "        #hidden : a tuple of 2 tensors of torch.Size([2, 48, 128])\n",
        "\n",
        "\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0] # torch.Size([48, 18, 256])\n",
        "        \n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        \n",
        "        words_emb = output.transpose(1, 2) #torch.Size([48, 256, 18])\n",
        "        \n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()#torch.Size([48, 2, 128])\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)#torch.Size([48, 256])\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
        "    \n",
        "    FONT_MAX = 50\n",
        "\n",
        "    num = captions.size(0)\n",
        "    img_txt = Image.fromarray(convas)\n",
        "    # get a font\n",
        "    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    print (\"CURRENT WORKING DIRCTORY : \" , os.getcwd())\n",
        "    fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    # get a drawing context\n",
        "    d = ImageDraw.Draw(img_txt)\n",
        "    sentence_list = []\n",
        "    for i in range(num):\n",
        "        cap = captions[i].data.cpu().numpy()\n",
        "        sentence = []\n",
        "        for j in range(len(cap)):\n",
        "            if cap[j] == 0:\n",
        "                break\n",
        "            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
        "            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
        "                   font=fnt, fill=(255, 255, 255, 255))\n",
        "            sentence.append(word)\n",
        "        sentence_list.append(sentence)\n",
        "    return img_txt, sentence_list\n",
        "\n",
        "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    w12 = torch.sum(x1 * x2, dim)\n",
        "    w1 = torch.norm(x1, 2, dim)\n",
        "    w2 = torch.norm(x2, 2, dim)\n",
        "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train() #Sets the module in training mode.\n",
        "    rnn_model.train() #Sets the module in training mode.\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # Loading the first batch (number of batches/steps in an epoch is 183)\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x 256 x 17 x 17 ==> # image region features\n",
        "        # sent_code: batch_size x 256                ==> # global image features\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)# 256, 17(16th of the whole image)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size) # A tuple of 2 zero tensor of torch.Size([2, 48, 128])\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    '''\n",
        "    RNN_ENCODER(\n",
        "    (encoder): Embedding(5450, 300)\n",
        "    (drop): Dropout(p=0.5, inplace=False)\n",
        "    (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True))\n",
        "    '''\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    '''\n",
        "    A tensor of [0,1,2,3,...,47]\n",
        "    '''\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    UPDATE_INTERVAL = 200\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = 299\n",
        "    batch_size = 48\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(355), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "    #using prepare data functiont this dataloader yieldes:\n",
        "    #imgs: a list of 1 tensor of size torch.Size([48, 3, 299, 299])\n",
        "    #captons: a  tensor of size torch.Size([48, 18]), shorter filled with end words converted by word to index\n",
        "    #cap_lens: a  tensor of size torch.Size([48]) , acual caps lens order from big to small (max is 18)\n",
        "    #class_ids: a 48 ints list in range 0-200 of the classes\n",
        "    #keys: a 48 string list  of the classes classes nammes crosspondening to the class_ids\n",
        "    \n",
        "\n",
        "    # # validation data #\n",
        "    dataset_val = TextDataset(cfg.DATA_DIR, 'test', base_size=cfg.TREE.BASE_SIZE,transform=image_transform)\n",
        "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, drop_last=True,shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters()) # 9 paramters\n",
        "    for v in image_encoder.parameters(): # 3 parameters\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR #0.002\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WqXJScC3TQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_loss0j, w_loss1j, attn_mapsj = words_loss(words_features, words_emb, labels, cap_lens, class_ids, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJkup0Qq5ect",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "for ten in attn_mapsj:\n",
        "  print(i)\n",
        "  print(ten.size())\n",
        "  i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2H5_hcC5Iw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_emb, sent_emb = text_encoder(captions, cap_lens, hidden)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFoONoAh3qQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_features, sent_code = image_encoder(imgs[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfMS1Mqf34Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step, data in enumerate(dataloader, 0):\n",
        "  if step == 0:  \n",
        "    image_encoder.zero_grad()\n",
        "    text_encoder.zero_grad()\n",
        "    imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gChJs6tW4NnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "print(dataset.n_words, dataset.embeddings_num)\n",
        "assert dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP35RynGxsKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = next(text_encoder.parameters()).data\n",
        "hidden= (Variable(weight.new(text_encoder.nlayers * text_encoder.num_directions,48, text_encoder.nhidden).zero_()),\n",
        "        Variable(weight.new(text_encoder.nlayers * text_encoder.num_directions,\n",
        "                            48, text_encoder.nhidden).zero_()))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thkZHPBp73qv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "88cadbfa-1bcc-4f81-ad91-92b142ee7619"
      },
      "source": [
        "for i in range(batch_size):\n",
        "    if class_ids is not None:\n",
        "        mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "        mask[i] = 0\n",
        "        masks.append(mask.reshape((1, -1)))\n",
        "    # Get the i-th text description\n",
        "    words_num = cap_lens[i]\n",
        "    # -> 1 x nef x words_num\n",
        "    word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "    # -> batch_size x nef x words_num\n",
        "    word = word.repeat(batch_size, 1, 1)\n",
        "    # batch x nef x 17*17\n",
        "    context = words_features\n",
        "    if i < 3:\n",
        "      print('words_num: ', words_num)\n",
        "      print('word: ', word.size())\n",
        "      print('context: ', context.size())\n",
        "    "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words_num:  18\n",
            "word:  torch.Size([48, 256, 18])\n",
            "context:  torch.Size([48, 256, 17, 17])\n",
            "words_num:  18\n",
            "word:  torch.Size([48, 256, 18])\n",
            "context:  torch.Size([48, 256, 17, 17])\n",
            "words_num:  18\n",
            "word:  torch.Size([48, 256, 18])\n",
            "context:  torch.Size([48, 256, 17, 17])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neg1dZi28Yyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "masks = []\n",
        "att_maps = []\n",
        "similarities = []"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8dVywvUyK_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "87c6d290-d94d-4812-966e-8bf1d60e772d"
      },
      "source": [
        "sent_emb.size()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3mQbbOkyE5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5bd7659b-d701-4136-a44e-0bb26fa03d59"
      },
      "source": [
        "words_emb.size()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNFW974OKg7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "1ed716c6-931e-401b-e42b-dc4815edc890"
      },
      "source": [
        "words_emb, sent_emb = text_encoder(captions, cap_lens, a)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-6f55dd65775d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-500e5e6e4a4f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, captions, cap_lens, hidden, mask)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;31m# Returns: a PackedSequence object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0mcap_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m#emb[0]: a tensor of torch.Size([660, 300])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9OWguWJQi2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jj = text_encoder.encoder(captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvCZPm4cR8hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jjj = text_encoder.drop(jj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la9N7vy3Q-JQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_encoder.encoder??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozTvpC2hPZiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ce8c9041-4d6a-4a43-86b6-41dff6e615e7"
      },
      "source": [
        "captions.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acp1ZbAoQtur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5c434493-b7ec-4afb-ee15-19f8b25b3033"
      },
      "source": [
        "jjj.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 18, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nMS1evePN-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e89d4b43-115a-402f-e3ca-183bdd2f2e94"
      },
      "source": [
        "words_emb.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQM9oCw3STTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pack_padded_sequence??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSwNY4OPUn4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputj, hiddenj = text_encoder.rnn(emb, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWoSp3PqU0YM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e6c3304e-8a46-4e66-ea43-3fa0ffaeaa9f"
      },
      "source": [
        "print(len(outputj))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwO3CbQnVeUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXLF-GOkVP4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "d62d838a-4876-43c9-9939-4158a4644549"
      },
      "source": [
        "for i,j in zip(emb[1], cap_lens):\n",
        "  print(i.item(),'<==>',j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "47 <==> 17\n",
            "45 <==> 17\n",
            "40 <==> 16\n",
            "25 <==> 16\n",
            "21 <==> 15\n",
            "17 <==> 15\n",
            "13 <==> 15\n",
            "11 <==> 15\n",
            "9 <==> 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLmP7WHXhQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputjj = pad_packed_sequence(outputj, batch_first=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-KfO7YBYFun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_emb = outputjj.transpose(1, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-qkrTJbYi8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_embj = hiddenj[0].transpose(0, 1).contiguous()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN4pqRB1Yw1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d7d29e66-b572-49d6-dbae-d56b402c0cf6"
      },
      "source": [
        "sent_embjj.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA9_I7QkZBVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_embjj = sent_embj.view(-1, text_encoder.nhidden * text_encoder.num_directions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMbhswa0Xmy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2bc70855-43ad-46d2-fae0-bc76fea79a9a"
      },
      "source": [
        "words_emb.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvK8XYogVPpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "29f95ac7-0c48-4b98-dbb2-846597dabae5"
      },
      "source": [
        "hiddenj[0].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 48, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7gE4hN5ScRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(outputj)\n",
        "for i, x in enumerate(outputj):\n",
        "  print(i)\n",
        "  print(x.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBKkjNFUPAJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "172aa013-53e0-4568-f1ec-0d261fad75b2"
      },
      "source": [
        "cap_lens = cap_lens.data.tolist()\n",
        "print(cap_lens)\n",
        "print(len(cap_lens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 16, 16, 15, 15, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 10, 10, 9]\n",
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ6GuQwHJ_04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebc99526-cede-44bf-ba4c-31c2f4a8a354"
      },
      "source": [
        "count = 1\n",
        "for i in captions:\n",
        "  print(count)\n",
        "  for j in i:\n",
        "    print(ixtoword[j.item()])\n",
        "  print('====================================')\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "this\n",
            "colorful\n",
            "bird\n",
            "has\n",
            "very\n",
            "long\n",
            "very\n",
            "thin\n",
            "beak\n",
            "feathers\n",
            "in\n",
            "vivid\n",
            "shades\n",
            "of\n",
            "blue\n",
            "purple\n",
            "and\n",
            "copper\n",
            "====================================\n",
            "2\n",
            "bird\n",
            "a\n",
            "yellow\n",
            "and\n",
            "white\n",
            "eyebrows\n",
            "mottled\n",
            "brown\n",
            "back\n",
            "and\n",
            "a\n",
            "yellow\n",
            "breast\n",
            "with\n",
            "a\n",
            "brown\n",
            "v\n",
            "shape\n",
            "====================================\n",
            "3\n",
            "this\n",
            "smaller\n",
            "bird\n",
            "has\n",
            "a\n",
            "long\n",
            "tail\n",
            "and\n",
            "feathers\n",
            "of\n",
            "yellow\n",
            "black\n",
            "and\n",
            "brown\n",
            "a\n",
            "short\n",
            "black\n",
            "beak\n",
            "====================================\n",
            "4\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "navy\n",
            "blue\n",
            "head\n",
            "throat\n",
            "back\n",
            "wings\n",
            "and\n",
            "tail\n",
            "and\n",
            "a\n",
            "light\n",
            "throat\n",
            "and\n",
            "belly\n",
            "====================================\n",
            "5\n",
            "this\n",
            "is\n",
            "a\n",
            "beautiful\n",
            "small\n",
            "white\n",
            "and\n",
            "blue\n",
            "bird\n",
            "with\n",
            "light\n",
            "blue\n",
            "wings\n",
            "and\n",
            "head\n",
            "and\n",
            "white\n",
            "belly\n",
            "====================================\n",
            "6\n",
            "this\n",
            "small\n",
            "bird\n",
            "contains\n",
            "a\n",
            "light\n",
            "yellow\n",
            "throat\n",
            "and\n",
            "breast\n",
            "green\n",
            "coverts\n",
            "and\n",
            "secondaries\n",
            "and\n",
            "a\n",
            "yellow\n",
            "ring\n",
            "====================================\n",
            "7\n",
            "this\n",
            "black\n",
            "white\n",
            "it\n",
            "has\n",
            "spots\n",
            "over\n",
            "it\n",
            "it\n",
            "has\n",
            "long\n",
            "legs\n",
            "tail\n",
            "feathers\n",
            "with\n",
            "a\n",
            "short\n",
            "beak\n",
            "====================================\n",
            "8\n",
            "this\n",
            "has\n",
            "a\n",
            "white\n",
            "crown\n",
            "and\n",
            "head\n",
            "long\n",
            "white\n",
            "neck\n",
            "orange\n",
            "bill\n",
            "and\n",
            "dark\n",
            "gray\n",
            "primaries\n",
            "and\n",
            "rectricles\n",
            "====================================\n",
            "9\n",
            "this\n",
            "is\n",
            "slender\n",
            "grey\n",
            "and\n",
            "black\n",
            "bird\n",
            "with\n",
            "a\n",
            "slim\n",
            "and\n",
            "long\n",
            "bill\n",
            "and\n",
            "black\n",
            "with\n",
            "grey\n",
            "belly\n",
            "====================================\n",
            "10\n",
            "this\n",
            "bird\n",
            "has\n",
            "grey\n",
            "green\n",
            "and\n",
            "black\n",
            "secondaries\n",
            "a\n",
            "grey\n",
            "head\n",
            "white\n",
            "eye\n",
            "ring\n",
            "and\n",
            "yellow\n",
            "breast\n",
            "<end>\n",
            "====================================\n",
            "11\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "a\n",
            "long\n",
            "slim\n",
            "black\n",
            "beak\n",
            "yellow\n",
            "belly\n",
            "a\n",
            "black\n",
            "crown\n",
            "and\n",
            "white\n",
            "retrices\n",
            "<end>\n",
            "====================================\n",
            "12\n",
            "this\n",
            "very\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "and\n",
            "green\n",
            "back\n",
            "this\n",
            "a\n",
            "long\n",
            "thin\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "13\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "eye\n",
            "ring\n",
            "belly\n",
            "and\n",
            "vent\n",
            "and\n",
            "speckled\n",
            "black\n",
            "and\n",
            "white\n",
            "secondaries\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "14\n",
            "this\n",
            "bird\n",
            "has\n",
            "black\n",
            "wings\n",
            "breast\n",
            "tail\n",
            "and\n",
            "feet\n",
            "and\n",
            "red\n",
            "white\n",
            "and\n",
            "black\n",
            "head\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "15\n",
            "this\n",
            "small\n",
            "brown\n",
            "green\n",
            "bird\n",
            "has\n",
            "dark\n",
            "round\n",
            "eyes\n",
            "and\n",
            "features\n",
            "a\n",
            "small\n",
            "thin\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "16\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "black\n",
            "overall\n",
            "body\n",
            "color\n",
            "even\n",
            "up\n",
            "to\n",
            "its\n",
            "feet\n",
            "and\n",
            "tarsus\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "17\n",
            "this\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "yellow\n",
            "body\n",
            "with\n",
            "a\n",
            "black\n",
            "breast\n",
            "and\n",
            "blue\n",
            "cheek\n",
            "patch\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "18\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "black\n",
            "secondaries\n",
            "white\n",
            "wing\n",
            "bars\n",
            "and\n",
            "a\n",
            "black\n",
            "cheek\n",
            "patch\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "19\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "black\n",
            "head\n",
            "a\n",
            "long\n",
            "black\n",
            "bill\n",
            "and\n",
            "a\n",
            "large\n",
            "wingspan\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "20\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "very\n",
            "large\n",
            "black\n",
            "and\n",
            "white\n",
            "fully\n",
            "head\n",
            "and\n",
            "orange\n",
            "eyes\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "21\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "brown\n",
            "crown\n",
            "as\n",
            "well\n",
            "as\n",
            "a\n",
            "brown\n",
            "and\n",
            "white\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "22\n",
            "this\n",
            "puffy\n",
            "bird\n",
            "has\n",
            "a\n",
            "patterned\n",
            "golden\n",
            "brown\n",
            "and\n",
            "white\n",
            "breast\n",
            "and\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "23\n",
            "the\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "brown\n",
            "body\n",
            "with\n",
            "black\n",
            "markings\n",
            "on\n",
            "its\n",
            "crown\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "24\n",
            "this\n",
            "bird\n",
            "is\n",
            "white\n",
            "black\n",
            "and\n",
            "has\n",
            "an\n",
            "orange\n",
            "spot\n",
            "on\n",
            "its\n",
            "side\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "25\n",
            "this\n",
            "bird\n",
            "is\n",
            "yellow\n",
            "white\n",
            "and\n",
            "brown\n",
            "in\n",
            "color\n",
            "with\n",
            "a\n",
            "black\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "26\n",
            "this\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "gray\n",
            "back\n",
            "and\n",
            "pointed\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "27\n",
            "this\n",
            "blue\n",
            "bird\n",
            "has\n",
            "a\n",
            "yellow\n",
            "eye\n",
            "black\n",
            "superciliary\n",
            "and\n",
            "black\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "28\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "brown\n",
            "and\n",
            "has\n",
            "a\n",
            "grey\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "29\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "brown\n",
            "and\n",
            "has\n",
            "a\n",
            "yellow\n",
            "throat\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "30\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "yellow\n",
            "body\n",
            "black\n",
            "head\n",
            "and\n",
            "short\n",
            "sharp\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "31\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "grey\n",
            "and\n",
            "has\n",
            "a\n",
            "yellow\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "32\n",
            "this\n",
            "bird\n",
            "is\n",
            "white\n",
            "and\n",
            "brown\n",
            "in\n",
            "color\n",
            "with\n",
            "a\n",
            "black\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "33\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "black\n",
            "and\n",
            "has\n",
            "an\n",
            "orange\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "34\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "long\n",
            "and\n",
            "pointy\n",
            "beak\n",
            "with\n",
            "a\n",
            "red\n",
            "nape\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "35\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "brown\n",
            "and\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "36\n",
            "a\n",
            "large\n",
            "white\n",
            "seabird\n",
            "with\n",
            "black\n",
            "secondaries\n",
            "and\n",
            "a\n",
            "sharp\n",
            "yellow\n",
            "bill\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "37\n",
            "a\n",
            "brightly\n",
            "colored\n",
            "red\n",
            "bird\n",
            "with\n",
            "a\n",
            "black\n",
            "tail\n",
            "and\n",
            "black\n",
            "wings\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "38\n",
            "the\n",
            "bird\n",
            "is\n",
            "dark\n",
            "black\n",
            "in\n",
            "color\n",
            "and\n",
            "has\n",
            "bright\n",
            "red\n",
            "eyes\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "39\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "black\n",
            "wing\n",
            "and\n",
            "a\n",
            "grey\n",
            "head\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "40\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "grey\n",
            "and\n",
            "has\n",
            "a\n",
            "yellow\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "41\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "bright\n",
            "multi\n",
            "coloring\n",
            "and\n",
            "red\n",
            "around\n",
            "eyes\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "42\n",
            "a\n",
            "small\n",
            "yellow\n",
            "bird\n",
            "with\n",
            "gray\n",
            "wings\n",
            "and\n",
            "a\n",
            "tan\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "43\n",
            "a\n",
            "brown\n",
            "bird\n",
            "with\n",
            "a\n",
            "black\n",
            "crown\n",
            "and\n",
            "a\n",
            "yellow\n",
            "throat\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "44\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "orange\n",
            "head\n",
            "a\n",
            "black\n",
            "and\n",
            "white\n",
            "body\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "45\n",
            "the\n",
            "bird\n",
            "is\n",
            "brown\n",
            "with\n",
            "a\n",
            "yellow\n",
            "eyering\n",
            "and\n",
            "small\n",
            "tarsals\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "46\n",
            "small\n",
            "white\n",
            "and\n",
            "mahogany\n",
            "bird\n",
            "with\n",
            "yellow\n",
            "throat\n",
            "and\n",
            "eyebrows\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "47\n",
            "the\n",
            "bird\n",
            "has\n",
            "a\n",
            "small\n",
            "black\n",
            "eyering\n",
            "and\n",
            "small\n",
            "bill\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "48\n",
            "a\n",
            "dark\n",
            "black\n",
            "bird\n",
            "with\n",
            "a\n",
            "black\n",
            "pointed\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X1Ojwf0MqoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "dcc42afc-3e86-4180-d693-141d0fbb4cab"
      },
      "source": [
        "filepath = os.path.join('/content', 'captions.pickle')\n",
        "with open(filepath, 'rb') as f:\n",
        "    x = pickle.load(f)\n",
        "    train_captions, test_captions = x[0], x[1]\n",
        "    ixtoword, wordtoix = x[2], x[3]\n",
        "    del x\n",
        "    n_words = len(ixtoword)\n",
        "    print('Load from: ', filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load from:  /content/captions.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcrt9BZrFWPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fb75f452-0e27-41fb-c0b7-de2a82fc3039"
      },
      "source": [
        "x = nn.Upsample(size=(299, 299), mode='bilinear')(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBI6bAeA-7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step, data in enumerate(dataloader, 0):\n",
        "  print('step ===> ', step)\n",
        "  imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "  print('size: ',imgs[-1].size(), 'first number: ', imgs[-1][0][0][0][0])\n",
        "  # words_features: batch_size x nef x 17 x 17\n",
        "  # sent_code: batch_size x nef\n",
        "  words_features, sent_code = image_encoder(imgs[-1])\n",
        "  print ('words_features', words_features.size())\n",
        "  print ('sent_code', sent_code.size())\n",
        "\n",
        "  print('================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcPYjphKieqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c87a8dae-d1f3-4a06-9db2-ee2231c965fd"
      },
      "source": [
        "import pickle\n",
        "filepath = '/content/filenames.pickle'\n",
        "with open(filepath, 'rb') as f:\n",
        "  filenames = pickle.load(f)\n",
        "print('Load filenames from: %s (%d)' % (filepath, len(filenames)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load filenames from: /content/filenames.pickle (8855)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlcltDDz6GGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "600339ad-1a4a-4993-ed66-d59114a49783"
      },
      "source": [
        "len(filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA7sAaTw4aQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_id = []\n",
        "for name in filenames:\n",
        "  labels_id.append(int(name[:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGUgGb5q6w0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "150762e2-baca-4dd1-dadd-d538f016bbe5"
      },
      "source": [
        "labels_id.index(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY2hYKJt475j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for id, name in zip(labels_id, filenames):\n",
        "  if id < 100:\n",
        "    print(id, '--', name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQjN2s_FjMFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b5b1c26e-75aa-4095-fb07-d6613b0f4b3f"
      },
      "source": [
        "int(filenames[0][:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IneuOKMd53GI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxSYzVY1i-YC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/__MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUz51myOh8RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKZBzQGSlWmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8cDTBH1mn1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(list(zip(labels, labels_id, sentences)), columns =['Labels', 'ID', 'Sentences']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeA6wQSX6fw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "60bf9a91-c7ad-4eb8-993d-3d4e9185ded1"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>ID</th>\n",
              "      <th>Sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>a bird with a very long wing span and a long p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>the long-beaked bird has a white body with lon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>this is a white bird with brown wings and a la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>this large bird has long bill, a white breast,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>bird has an extremely long wingspan with a dar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8850</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>the crown of the bird is brown the body is bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8851</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>a dark brown bird with black spots, light brow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8852</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>this bird is brown and black in color with a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8853</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>the bird has an oval shaped, small eye and the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8854</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>this bird has a flat head an extremely small b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8855 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Labels  ...                                          Sentences\n",
              "0       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  a bird with a very long wing span and a long p...\n",
              "1       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  the long-beaked bird has a white body with lon...\n",
              "2       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  this is a white bird with brown wings and a la...\n",
              "3       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  this large bird has long bill, a white breast,...\n",
              "4       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  bird has an extremely long wingspan with a dar...\n",
              "...                                                 ...  ...                                                ...\n",
              "8850  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  the crown of the bird is brown the body is bro...\n",
              "8851  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  a dark brown bird with black spots, light brow...\n",
              "8852  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  this bird is brown and black in color with a s...\n",
              "8853  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  the bird has an oval shaped, small eye and the...\n",
              "8854  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  this bird has a flat head an extremely small b...\n",
              "\n",
              "[8855 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzBVsgyNnEPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('CUB_captions.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1kBfX7skRoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "all_captions = []\n",
        "for i in range(len(filenames)):\n",
        "    cap_path = '/content/text/%s.txt' % ( filenames[i])\n",
        "    with open(cap_path, \"r\") as f:\n",
        "        captions = f.read().split('\\n')\n",
        "        if i < 8900:\n",
        "          print(i)\n",
        "          for j in captions :\n",
        "            if j=='':\n",
        "              continue\n",
        "            sentences.append(j)\n",
        "            labels.append(filenames[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "49beea18-613d-4d89-85b1-9fdb33866752"
      },
      "source": [
        "all_captions = []\n",
        "for i in range(len(filenames)):\n",
        "    cap_path = '/content/text/%s.txt' % ( filenames[i])\n",
        "    with open(cap_path, \"r\") as f:\n",
        "        captions = f.read().decode('utf8').split('\\n')\n",
        "        cnt = 0\n",
        "        for cap in captions:\n",
        "            if len(cap) == 0:\n",
        "                continue\n",
        "            cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "            # picks out sequences of alphanumeric characters as tokens\n",
        "            # and drops everything else\n",
        "            tokenizer = RegexpTokenizer(r'\\w+')\n",
        "            tokens = tokenizer.tokenize(cap.lower())\n",
        "            # print('tokens', tokens)\n",
        "            if len(tokens) == 0:\n",
        "                print('cap', cap)\n",
        "                continue\n",
        "\n",
        "            tokens_new = []\n",
        "            for t in tokens:\n",
        "                t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                if len(t) > 0:\n",
        "                    tokens_new.append(t)\n",
        "            all_captions.append(tokens_new)\n",
        "            cnt += 1\n",
        "            if cnt == self.embeddings_num:\n",
        "                break\n",
        "        if cnt < self.embeddings_num:\n",
        "            print('ERROR: the captions for %s less than %d'\n",
        "                  % (filenames[i], cnt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPsXrpHhdgM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QjAGCOOqNtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtp95TB6qWdC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "75c7fec6-da5e-4baa-8282-74e2ae144ee6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/CUB_captions.csv\", header=None, names=['Label',  'ID', 'Sentences'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,856\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>ID</th>\n",
              "      <th>Sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2605.0</th>\n",
              "      <td>010.Red_winged_Blackbird/Red_Winged_Blackbird_...</td>\n",
              "      <td>64</td>\n",
              "      <td>this bird is black with red and has a long, po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5003.0</th>\n",
              "      <td>015.Lazuli_Bunting/Lazuli_Bunting_0082_15047</td>\n",
              "      <td>122</td>\n",
              "      <td>a small bird with a blue head and yellow belly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3897.0</th>\n",
              "      <td>012.Yellow_headed_Blackbird/Yellow_Headed_Blac...</td>\n",
              "      <td>93</td>\n",
              "      <td>a small bird with a yellow head and breast alo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5931.0</th>\n",
              "      <td>017.Cardinal/Cardinal_0029_17297</td>\n",
              "      <td>140</td>\n",
              "      <td>small bird with upright reddish feathers in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7646.0</th>\n",
              "      <td>020.Yellow_breasted_Chat/Yellow_Breasted_Chat_...</td>\n",
              "      <td>174</td>\n",
              "      <td>this bird has a white belly, a yellow breast a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492.0</th>\n",
              "      <td>005.Crested_Auklet/Crested_Auklet_0076_785252</td>\n",
              "      <td>42</td>\n",
              "      <td>a black body, white eye with stripe next to it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257.0</th>\n",
              "      <td>005.Crested_Auklet/Crested_Auklet_0044_1825</td>\n",
              "      <td>32</td>\n",
              "      <td>this bird is grey with blue and has a very sho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218.0</th>\n",
              "      <td>005.Crested_Auklet/Crested_Auklet_0018_1817</td>\n",
              "      <td>30</td>\n",
              "      <td>this bird is gray and brown in color, and has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421.0</th>\n",
              "      <td>010.Red_winged_Blackbird/Red_Winged_Blackbird_...</td>\n",
              "      <td>61</td>\n",
              "      <td>small black bird with a short black beak and b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8703.0</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0047_796971</td>\n",
              "      <td>198</td>\n",
              "      <td>a stout, brown bird speckled with white with e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Label  ...                                          Sentences\n",
              "2605.0  010.Red_winged_Blackbird/Red_Winged_Blackbird_...  ...  this bird is black with red and has a long, po...\n",
              "5003.0       015.Lazuli_Bunting/Lazuli_Bunting_0082_15047  ...  a small bird with a blue head and yellow belly...\n",
              "3897.0  012.Yellow_headed_Blackbird/Yellow_Headed_Blac...  ...  a small bird with a yellow head and breast alo...\n",
              "5931.0                   017.Cardinal/Cardinal_0029_17297  ...  small bird with upright reddish feathers in th...\n",
              "7646.0  020.Yellow_breasted_Chat/Yellow_Breasted_Chat_...  ...  this bird has a white belly, a yellow breast a...\n",
              "1492.0      005.Crested_Auklet/Crested_Auklet_0076_785252  ...  a black body, white eye with stripe next to it...\n",
              "1257.0        005.Crested_Auklet/Crested_Auklet_0044_1825  ...  this bird is grey with blue and has a very sho...\n",
              "1218.0        005.Crested_Auklet/Crested_Auklet_0018_1817  ...  this bird is gray and brown in color, and has ...\n",
              "2421.0  010.Red_winged_Blackbird/Red_Winged_Blackbird_...  ...  small black bird with a short black beak and b...\n",
              "8703.0  022.Chuck_will_Widow/Chuck_Will_Widow_0047_796971  ...  a stout, brown bird speckled with white with e...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ZKbuVkqwdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "import numpy as np\n",
        "\n",
        "sentences = df.Sentences.values\n",
        "labels_text = df.Label.values\n",
        "labels = df.ID.values\n",
        "\n",
        "labels = np.delete(labels, 0)\n",
        "labels_text = np.delete(labels_text, 0)\n",
        "sentences = np.delete(sentences, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT4m4nzZrWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = labels.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sASty7Vl8zjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ff6a67a5-3554-4c74-fdc6-52c81fc3f2dc"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  2,   2,   2,  ..., 200, 200, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoScn6RP8u8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ef8bd9e6-981a-492e-c5de-6fda18a8ede0"
      },
      "source": [
        "import torch\n",
        "print (labels)\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  2   2   2 ... 200 200 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC8iMadFrbfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWREbAAF8Uvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYL7RPBbsw9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2ac9afc6-b5a8-4142-8f5e-c59835748bbc"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  a bird with a very long wing span and a long pointed beak.\n",
            "Tokenized:  ['a', 'bird', 'with', 'a', 'very', 'long', 'wing', 'span', 'and', 'a', 'long', 'pointed', 'beak', '.']\n",
            "Token IDs:  [1037, 4743, 2007, 1037, 2200, 2146, 3358, 8487, 1998, 1037, 2146, 4197, 23525, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ufKmKoRs2Nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "4a11a83a-0562-46bd-b3a8-63831d6b4bb3"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    if  len(input_ids) == 80:\n",
        "      print(sent)\n",
        "      print(len(sent.split()))\n",
        "      print(sentences.index(sent))\n",
        "\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1914edf96f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# For every sentence...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Tokenize the text and add `[CLS]` and `[SEP]` tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZhTczPhtTqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atwal = []\n",
        "for i in sentences:\n",
        "  atwal.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZPTynGpt45_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atwal.sort(reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnIqqmdtu2Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atwal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUizQ2wrv-3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation=True\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dqg9xc7x5Ax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "0842e68b-cefc-4c8c-86f9-31c484ecc870"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 16 00:31:14 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    34W /  70W |   4877MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4X71caDCDkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW1_ZwAzC7t6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "fa03775b29974417b41fbd64953dfe8e",
            "4ad38c15dadd4b2a90e4d8761121b272",
            "5d50187b34a447278202d1b6ea86ceb2",
            "88792eb2b0354713923855af0f200bff",
            "28ebfd706ae34cd7a4d3809e3ce07e1a",
            "2b85a14c40404f5ba263eec7b18b39c5",
            "f1df89e12ec64d90aeb4e393a9b6b1c5",
            "d7071ad2f1954960922567dea36308fa"
          ]
        },
        "outputId": "fcd73dab-eaa5-45ce-d8d4-1a55429eef2b"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140529629273896 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpy9sle09b\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa03775b29974417b41fbd64953dfe8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:filelock:Lock 140529629273896 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAJWqw7dDmV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "030b5c9c-6bc2-45d4-ef5d-d8ab73fb7f36"
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1ZYH0UoEDwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c4e89e13-d780-40aa-ff62-b1f36da510c7"
      },
      "source": [
        "list(tokenizer.vocab.keys())[5000:5020]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['knight',\n",
              " 'lap',\n",
              " 'survey',\n",
              " 'ma',\n",
              " '##ow',\n",
              " 'noise',\n",
              " 'billy',\n",
              " '##ium',\n",
              " 'shooting',\n",
              " 'guide',\n",
              " 'bedroom',\n",
              " 'priest',\n",
              " 'resistance',\n",
              " 'motor',\n",
              " 'homes',\n",
              " 'sounded',\n",
              " 'giant',\n",
              " '##mer',\n",
              " '150',\n",
              " 'scenes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFXUjY-iE3BP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBEHrN8qFSG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "beeb72ed-2905-4f9e-92a6-9bea74d981a4"
      },
      "source": [
        "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ8lg_GJHNr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwmAY7hdHPHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbl6I8TdFqlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmLkohJhMXpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7b14c22b-5feb-43f8-e54d-07e3dd8d4918"
      },
      "source": [
        "print(segments_tensors.size())"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 22])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vrVSBhgMJfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ea7d7baf-d19e-4e47-81ef-d82819d07ca4"
      },
      "source": [
        "print(tokens_tensor.size())"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 22])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00i1UbfDHOX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ZeeqBoHOMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "    # Evaluating the model will return a different number of objects based on \n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "    # becase we set `output_hidden_states = True`, the third item will be the \n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = outputs[2]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G86ZPiRoFqh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "3f1ddea4-fe3b-4f7b-a129-7c5151f7fba8"
      },
      "source": [
        "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbywsbK0H2BG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "112838fb-4bf5-4dbd-d0c1-b2d72275ccc8"
      },
      "source": [
        "# For the 5th token in our sentence, select its feature values from layer 5.\n",
        "token_i = 5\n",
        "layer_i = 5\n",
        "vec = hidden_states[layer_i][batch_i][token_i]\n",
        "\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oECodFt9H1xn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "477cea99-9e7b-46e3-ba4f-9ad831a8fd90"
      },
      "source": [
        "# `hidden_states` is a Python list.\n",
        "print('      Type of hidden_states: ', type(hidden_states))\n",
        "\n",
        "# Each layer in the list is a torch tensor.\n",
        "print('Tensor shape for each layer: ', hidden_states[0].size())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Type of hidden_states:  <class 'tuple'>\n",
            "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yIOGZ82JGt_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7d1e369d-cf8f-4585-c28f-4c8817e95dd2"
      },
      "source": [
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 1, 22, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhFz43ggJGcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2148dfac-b0e1-41da-e6b0-7eaa206806bc"
      },
      "source": [
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 22, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632pTnT9JGSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5b38d4a3-1e56-404f-b126-41f42d00c715"
      },
      "source": [
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 13, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChwZpkGSJGGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f37ad630-ae12-41a2-ab7a-06e08969e0e1"
      },
      "source": [
        "# Stores the token vectors, with shape [22 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 22 x 3072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN1S1ZUyKhkP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "64935fa8-1cd4-4161-8cfe-a8c85e9e9da0"
      },
      "source": [
        "# Stores the token vectors, with shape [22 x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 22 x 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYFHuC_RKhhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [22 x 768]\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF8RVJ0jKheM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TFKU51dJF1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXqlSVcdFrPH",
        "colab_type": "text"
      },
      "source": [
        "# ----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRrXuBeBFqfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d2928ebb-76ed-463b-ff6e-95758fceeeff"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZAMbSKgF1PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0yfc6A9F7by",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/CUB_captions.csv\", header=None, names=['Label',  'ID', 'Sentences'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHusoAxnF_Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "import numpy as np\n",
        "\n",
        "sentences = df.Sentences.values\n",
        "labels_text = df.Label.values\n",
        "labels = df.ID.values\n",
        "\n",
        "labels = np.delete(labels, 0)\n",
        "labels_text = np.delete(labels_text, 0)\n",
        "sentences = np.delete(sentences, 0)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mXa7igNGDMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4f15a6e8-12cc-45ec-f440-ffebef45bebc"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN3oAO2xGQ_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5b606f60-ce7c-4971-c8b6-65addacb810c"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    if  len(input_ids) == 80:\n",
        "      print('sent with max len :', sent)\n",
        "      print('number of words insent with max len :', len(sent.split()))\n",
        "      print('index of sent with max len :', np.where(sentences==sent))\n",
        "\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "atwal = []\n",
        "for i in sentences:\n",
        "  atwal.append(len(i.split()))\n",
        "atwal.sort(reverse=True)\n",
        "print('Max number of words in a sentnces length: ', atwal[0])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  70\n",
            "Max number of words in a sentnces length:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h-l0Nq6GXYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "8c6b1d9e-5f32-4d54-885b-186912b1681f"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation=True\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = labels.astype(int)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "segments_ids = []\n",
        "for i in range(input_ids.size()[0]):\n",
        "  segments_id = [1] * input_ids.size()[1]\n",
        "  segments_ids.append(segments_id)\n",
        "\n",
        "segments_ids = torch.tensor(segments_ids)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  a bird with a very long wing span and a long pointed beak.\n",
            "Token IDs: tensor([  101,  1037,  4743,  2007,  1037,  2200,  2146,  3358,  8487,  1998,\n",
            "         1037,  2146,  4197, 23525,  1012,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAVvXBl-bO4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5fd0aac0-a7a2-4d5b-9e8c-1535e641bc59"
      },
      "source": [
        "segments_ids.size()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8855, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ov14uwOGkqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c6930c19-e43f-49cc-f22b-7b1759289cb2"
      },
      "source": [
        "input_ids.size()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8855, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb9hE2dQOHeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, segments_ids, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(1 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUpKpzknOWtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW-2lD5gOjU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsgU2yCbPafe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i--NWiP9Pl65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4ccc000-ac34-41d3-9136-399d245e456d"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "k = 0\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  if step % 40 == 0 :  \n",
        "    # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "    # `to` method.\n",
        "    #\n",
        "    # `batch` contains three pytorch tensors:\n",
        "    #   [0]: input ids \n",
        "    #   [1]: attention masks\n",
        "    #   [2]: labels \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_segments_ids = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    print(device,'=> ', step)\n",
        "    print(\"b_segments_ids.size()\")\n",
        "    print(b_segments_ids.size())\n",
        "    print(\"b_input_ids.size()\")\n",
        "    print(b_input_ids.size())\n",
        "    #print(b_segments_ids[0])\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "          outputs = model(b_input_ids, b_segments_ids)\n",
        "          hidden_states = outputs[2]\n",
        "    \n",
        "\n",
        "    print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "    layer_i = 0\n",
        "    print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "    batch_i = 0\n",
        "    print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "    token_i = 0\n",
        "    print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
        "\n",
        "\n",
        "    # Concatenate the tensors for all layers. We use `stack` here to\n",
        "    # create a new dimension in the tensor.\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "    print('token_embeddings.size()', token_embeddings.size())\n",
        "\n",
        "\n",
        "    # Swap dimensions 0 and 1.\n",
        "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
        "    print('token_embeddings.size()', token_embeddings.size())\n",
        "\n",
        "\n",
        "    word_vecs_sum = []\n",
        "    for sentence in token_embeddings:\n",
        "      # `sentence` is a [64 x 12 x 768] tensor.\n",
        "      \n",
        "      # Stores the token vectors, with shape [64 x 768]\n",
        "      token_vecs_sum = []\n",
        "      # For each token in the sentence...\n",
        "      \n",
        "      for token in sentence:\n",
        "        # `token` is a [12 x 768] tensor\n",
        "        # Sum the vectors from the last four layers.\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "        # Use `sum_vec` to represent `token`.\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "\n",
        "      # Use `token_vecs_sum` to represent `sentence`.\n",
        "      word_vecs_sum.append(token_vecs_sum)\n",
        "\n",
        "    print ('Shape is: %d x %d x %d' % (len(word_vecs_sum), len(word_vecs_sum[0]), len(word_vecs_sum[0][0])))\n",
        "\n",
        "\n",
        "    # `hidden_states` has shape [13 x 32 x 64 x 768]\n",
        "    # `token_vecs` is a tensor with shape [64 x 768]\n",
        "    token_vecs = hidden_states[-2]\n",
        "\n",
        "    # Calculate the average of all 64 token vectors.\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=1)\n",
        "    print('sentence_embedding.size()', sentence_embedding.size())\n"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda =>  0\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n",
            "cuda =>  40\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n",
            "cuda =>  80\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n",
            "cuda =>  120\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n",
            "cuda =>  160\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n",
            "cuda =>  200\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n",
            "cuda =>  240\n",
            "b_segments_ids.size()\n",
            "torch.Size([32, 64])\n",
            "b_input_ids.size()\n",
            "torch.Size([32, 64])\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 32\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "token_embeddings.size() torch.Size([13, 32, 64, 768])\n",
            "token_embeddings.size() torch.Size([32, 64, 13, 768])\n",
            "Shape is: 32 x 64 x 768\n",
            "sentence_embedding.size() torch.Size([32, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szoCOSp4XrbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}