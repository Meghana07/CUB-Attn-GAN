{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextEncoderCubAttn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SJ2aHTi7QUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data\n",
        "\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\" -O CUB_200_2011.tgz && rm -rf /tmp/cookies.txt\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gONfh1Fi2eL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96f0432a-37dc-4fe5-8b38-aa3c7b60d936"
      },
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from copy import deepcopy\n",
        "import skimage.transform\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    \n",
        "    COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
        "                2:[70, 70, 70],  3:[102,102,156],\n",
        "                4:[190,153,153], 5:[153,153,153],\n",
        "                6:[250,170, 30], 7:[220, 220, 0],\n",
        "                8:[107,142, 35], 9:[152,251,152],\n",
        "                10:[70,130,180], 11:[220,20, 60],\n",
        "                12:[255, 0, 0],  13:[0, 0, 142],\n",
        "                14:[119,11, 32], 15:[0, 60,100],\n",
        "                16:[0, 80, 100], 17:[0, 0, 230],\n",
        "                18:[0,  0, 70],  19:[0, 0,  0]}\n",
        "    FONT_MAX = 50\n",
        "\n",
        "    \n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []# [299]\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "        print(\"self.imsize\", self.imsize)\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox() # 11788 long dictionry with key as image name and value is 4 ints list bounding box\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "        #filenames: List of 8855 text items of image names\n",
        "        #captions: List of 885 varible lengths captions -in range 9-18 -\n",
        "        #ixtoword: dictionry  of 5450 index [key] to word [value] pairs\n",
        "        #wordtoix: dictionry  of 5450 word [key] to index [value] pairs\n",
        "        #n_words: 5450\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames)) #200 classes, len:8855\n",
        "\n",
        "        self.number_example = len(self.filenames) #8855\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                    nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM # max length and padded to captions= 18\n",
        "        self.ntoken = ntoken  # size of the dictionary = 5450\n",
        "        self.ninput = ninput  # size of each embedding vector = 300\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed = 0.5\n",
        "        self.nlayers = nlayers  # Number of recurrent layers =1\n",
        "        self.bidirectional = bidirectional # True\n",
        "        self.rnn_type = cfg.RNN_TYPE #LSTM\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions # 128\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                                self.nlayers, batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                                self.nlayers, batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions, bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        #emb[0]: a tensor of torch.Size([660, 300])\n",
        "        #emb[1]: a tensor of torch.Size([18])\n",
        "        #emb[2]: None\n",
        "        #emb[3]: None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "\n",
        "\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        #output[0]: a tensor of torch.Size([660, 256])\n",
        "        #output[1]: a tensor of torch.Size([18])\n",
        "        #output[2]: None\n",
        "        #output[3]: None\n",
        "        #hidden : a tuple of 2 tensors of torch.Size([2, 48, 128])\n",
        "\n",
        "\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0] # torch.Size([48, 18, 256])\n",
        "        \n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        \n",
        "        words_emb = output.transpose(1, 2) #torch.Size([48, 256, 18])\n",
        "        \n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()#torch.Size([48, 2, 128])\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)#torch.Size([48, 256])\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
        "    \n",
        "    FONT_MAX = 50\n",
        "\n",
        "    num = captions.size(0)\n",
        "    img_txt = Image.fromarray(convas)\n",
        "    # get a font\n",
        "    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    print (\"CURRENT WORKING DIRCTORY : \" , os.getcwd())\n",
        "    fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    # get a drawing context\n",
        "    d = ImageDraw.Draw(img_txt)\n",
        "    sentence_list = []\n",
        "    for i in range(num):\n",
        "        cap = captions[i].data.cpu().numpy()\n",
        "        sentence = []\n",
        "        for j in range(len(cap)):\n",
        "            if cap[j] == 0:\n",
        "                break\n",
        "            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
        "            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
        "                   font=fnt, fill=(255, 255, 255, 255))\n",
        "            sentence.append(word)\n",
        "        sentence_list.append(sentence)\n",
        "    return img_txt, sentence_list\n",
        "\n",
        "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    w12 = torch.sum(x1 * x2, dim)\n",
        "    w1 = torch.norm(x1, 2, dim)\n",
        "    w2 = torch.norm(x2, 2, dim)\n",
        "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train() #Sets the module in training mode.\n",
        "    rnn_model.train() #Sets the module in training mode.\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # Loading the first batch (number of batches/steps in an epoch is 183)\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x 256 x 17 x 17 ==> # image region features\n",
        "        # sent_code: batch_size x 256                ==> # global image features\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)# 256, 17(16th of the whole image)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size) # A tuple of 2 zero tensor of torch.Size([2, 48, 128])\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    '''\n",
        "    RNN_ENCODER(\n",
        "    (encoder): Embedding(5450, 300)\n",
        "    (drop): Dropout(p=0.5, inplace=False)\n",
        "    (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True))\n",
        "    '''\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    '''\n",
        "    A tensor of [0,1,2,3,...,47]\n",
        "    '''\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    UPDATE_INTERVAL = 200\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = 299\n",
        "    batch_size = 48\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(355), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "    #using prepare data functiont this dataloader yieldes:\n",
        "    #imgs: a list of 1 tensor of size torch.Size([48, 3, 299, 299])\n",
        "    #captons: a  tensor of size torch.Size([48, 18]), shorter filled with end words converted by word to index\n",
        "    #cap_lens: a  tensor of size torch.Size([48]) , acual caps lens order from big to small (max is 18)\n",
        "    #class_ids: a 48 ints list in range 0-200 of the classes\n",
        "    #keys: a 48 string list  of the classes classes nammes crosspondening to the class_ids\n",
        "    \n",
        "\n",
        "    # # validation data #\n",
        "    dataset_val = TextDataset(cfg.DATA_DIR, 'test', base_size=cfg.TREE.BASE_SIZE,transform=image_transform)\n",
        "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, drop_last=True,shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters()) # 9 paramters\n",
        "    for v in image_encoder.parameters(): # 3 parameters\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR #0.002\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'DAMSM',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 128,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 48,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '',\n",
            "           'NET_G': '',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 1.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
            " 'WORKERS': 1}\n",
            "self.imsize [299]\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "5450 10\n",
            "self.imsize [299]\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "keyword |||||||||||||||||||||||||||||||\n",
            "Start_epoch :  0\n",
            "cfg.TRAIN.MAX_EPOCH :  600\n",
            "keyword |||||||||||||||||||||||||||||||\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1017: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1026: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1090: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "s_total_loss0.item()/UPDATE_INTERVAL :  0.019507784843444825\n",
            "s_total_loss1.item()/UPDATE_INTERVAL :  0.01979528546333313\n",
            "w_total_loss0.item()/UPDATE_INTERVAL :  0.02676588773727417\n",
            "w_total_loss1.item()/UPDATE_INTERVAL :  0.021168949604034423\n",
            "| epoch   0 |     0/  184 batches | ms/batch  7.38 | s_loss  0.02  0.02 | w_loss  0.03  0.02\n",
            "bulding images\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  239.9231297969818\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "train_function_time :  383.71231150627136\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end epoch   0 | valid loss  6.63  6.42 | lr 0.00200|\n",
            "-----------------------------------------------------------------------------------------\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "one_epoch_time :  418.31836581230164\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "Save G/Ds models.\n",
            "s_total_loss0.item()/UPDATE_INTERVAL :  0.015911502838134764\n",
            "s_total_loss1.item()/UPDATE_INTERVAL :  0.016014758348464966\n",
            "w_total_loss0.item()/UPDATE_INTERVAL :  0.01597333312034607\n",
            "w_total_loss1.item()/UPDATE_INTERVAL :  0.015303376913070679\n",
            "| epoch   1 |     0/  184 batches | ms/batch  6.72 | s_loss  0.02  0.02 | w_loss  0.02  0.02\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "train_function_time :  131.5541000366211\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end epoch   1 | valid loss  6.14  5.86 | lr 0.00196|\n",
            "-----------------------------------------------------------------------------------------\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "one_epoch_time :  165.29966688156128\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "s_total_loss0.item()/UPDATE_INTERVAL :  0.014769824743270875\n",
            "s_total_loss1.item()/UPDATE_INTERVAL :  0.014745883941650391\n",
            "w_total_loss0.item()/UPDATE_INTERVAL :  0.013174195289611817\n",
            "w_total_loss1.item()/UPDATE_INTERVAL :  0.013379864692687989\n",
            "| epoch   2 |     0/  184 batches | ms/batch  6.65 | s_loss  0.01  0.01 | w_loss  0.01  0.01\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "train_function_time :  131.81065559387207\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end epoch   2 | valid loss  6.05  5.60 | lr 0.00192|\n",
            "-----------------------------------------------------------------------------------------\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "one_epoch_time :  165.93257975578308\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "s_total_loss0.item()/UPDATE_INTERVAL :  0.013512510061264037\n",
            "s_total_loss1.item()/UPDATE_INTERVAL :  0.013550721406936646\n",
            "w_total_loss0.item()/UPDATE_INTERVAL :  0.012996338605880738\n",
            "w_total_loss1.item()/UPDATE_INTERVAL :  0.013036030530929565\n",
            "| epoch   3 |     0/  184 batches | ms/batch  6.70 | s_loss  0.01  0.01 | w_loss  0.01  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP35RynGxsKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = next(text_encoder.parameters()).data\n",
        "a= (Variable(weight.new(text_encoder.nlayers * text_encoder.num_directions,48, text_encoder.nhidden).zero_()),\n",
        "        Variable(weight.new(text_encoder.nlayers * text_encoder.num_directions,\n",
        "                            48, text_encoder.nhidden).zero_()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNFW974OKg7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a89fbf16-f4b0-4db0-9afc-839974a742c1"
      },
      "source": [
        "words_emb, sent_emb = text_encoder(captions, cap_lens, a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-6f55dd65775d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a2859b5b0eba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, captions, cap_lens, hidden, mask)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# Returns: a PackedSequence object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mcap_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;31m# #hidden and memory (num_layers * num_directions, batch, hidden_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9OWguWJQi2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jj = text_encoder.encoder(captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvCZPm4cR8hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jjj = text_encoder.drop(jj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la9N7vy3Q-JQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_encoder.encoder??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozTvpC2hPZiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ce8c9041-4d6a-4a43-86b6-41dff6e615e7"
      },
      "source": [
        "captions.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acp1ZbAoQtur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5c434493-b7ec-4afb-ee15-19f8b25b3033"
      },
      "source": [
        "jjj.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 18, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nMS1evePN-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e89d4b43-115a-402f-e3ca-183bdd2f2e94"
      },
      "source": [
        "words_emb.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQM9oCw3STTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pack_padded_sequence??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSwNY4OPUn4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputj, hiddenj = text_encoder.rnn(emb, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWoSp3PqU0YM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e6c3304e-8a46-4e66-ea43-3fa0ffaeaa9f"
      },
      "source": [
        "print(len(outputj))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwO3CbQnVeUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXLF-GOkVP4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "d62d838a-4876-43c9-9939-4158a4644549"
      },
      "source": [
        "for i,j in zip(emb[1], cap_lens):\n",
        "  print(i.item(),'<==>',j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "48 <==> 18\n",
            "47 <==> 17\n",
            "45 <==> 17\n",
            "40 <==> 16\n",
            "25 <==> 16\n",
            "21 <==> 15\n",
            "17 <==> 15\n",
            "13 <==> 15\n",
            "11 <==> 15\n",
            "9 <==> 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLmP7WHXhQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputjj = pad_packed_sequence(outputj, batch_first=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-KfO7YBYFun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_emb = outputjj.transpose(1, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-qkrTJbYi8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_embj = hiddenj[0].transpose(0, 1).contiguous()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN4pqRB1Yw1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d7d29e66-b572-49d6-dbae-d56b402c0cf6"
      },
      "source": [
        "sent_embjj.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA9_I7QkZBVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_embjj = sent_embj.view(-1, text_encoder.nhidden * text_encoder.num_directions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMbhswa0Xmy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2bc70855-43ad-46d2-fae0-bc76fea79a9a"
      },
      "source": [
        "words_emb.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvK8XYogVPpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "29f95ac7-0c48-4b98-dbb2-846597dabae5"
      },
      "source": [
        "hiddenj[0].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 48, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7gE4hN5ScRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(outputj)\n",
        "for i, x in enumerate(outputj):\n",
        "  print(i)\n",
        "  print(x.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBKkjNFUPAJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "172aa013-53e0-4568-f1ec-0d261fad75b2"
      },
      "source": [
        "cap_lens = cap_lens.data.tolist()\n",
        "print(cap_lens)\n",
        "print(len(cap_lens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 16, 16, 15, 15, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 10, 10, 9]\n",
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ6GuQwHJ_04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebc99526-cede-44bf-ba4c-31c2f4a8a354"
      },
      "source": [
        "count = 1\n",
        "for i in captions:\n",
        "  print(count)\n",
        "  for j in i:\n",
        "    print(ixtoword[j.item()])\n",
        "  print('====================================')\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "this\n",
            "colorful\n",
            "bird\n",
            "has\n",
            "very\n",
            "long\n",
            "very\n",
            "thin\n",
            "beak\n",
            "feathers\n",
            "in\n",
            "vivid\n",
            "shades\n",
            "of\n",
            "blue\n",
            "purple\n",
            "and\n",
            "copper\n",
            "====================================\n",
            "2\n",
            "bird\n",
            "a\n",
            "yellow\n",
            "and\n",
            "white\n",
            "eyebrows\n",
            "mottled\n",
            "brown\n",
            "back\n",
            "and\n",
            "a\n",
            "yellow\n",
            "breast\n",
            "with\n",
            "a\n",
            "brown\n",
            "v\n",
            "shape\n",
            "====================================\n",
            "3\n",
            "this\n",
            "smaller\n",
            "bird\n",
            "has\n",
            "a\n",
            "long\n",
            "tail\n",
            "and\n",
            "feathers\n",
            "of\n",
            "yellow\n",
            "black\n",
            "and\n",
            "brown\n",
            "a\n",
            "short\n",
            "black\n",
            "beak\n",
            "====================================\n",
            "4\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "navy\n",
            "blue\n",
            "head\n",
            "throat\n",
            "back\n",
            "wings\n",
            "and\n",
            "tail\n",
            "and\n",
            "a\n",
            "light\n",
            "throat\n",
            "and\n",
            "belly\n",
            "====================================\n",
            "5\n",
            "this\n",
            "is\n",
            "a\n",
            "beautiful\n",
            "small\n",
            "white\n",
            "and\n",
            "blue\n",
            "bird\n",
            "with\n",
            "light\n",
            "blue\n",
            "wings\n",
            "and\n",
            "head\n",
            "and\n",
            "white\n",
            "belly\n",
            "====================================\n",
            "6\n",
            "this\n",
            "small\n",
            "bird\n",
            "contains\n",
            "a\n",
            "light\n",
            "yellow\n",
            "throat\n",
            "and\n",
            "breast\n",
            "green\n",
            "coverts\n",
            "and\n",
            "secondaries\n",
            "and\n",
            "a\n",
            "yellow\n",
            "ring\n",
            "====================================\n",
            "7\n",
            "this\n",
            "black\n",
            "white\n",
            "it\n",
            "has\n",
            "spots\n",
            "over\n",
            "it\n",
            "it\n",
            "has\n",
            "long\n",
            "legs\n",
            "tail\n",
            "feathers\n",
            "with\n",
            "a\n",
            "short\n",
            "beak\n",
            "====================================\n",
            "8\n",
            "this\n",
            "has\n",
            "a\n",
            "white\n",
            "crown\n",
            "and\n",
            "head\n",
            "long\n",
            "white\n",
            "neck\n",
            "orange\n",
            "bill\n",
            "and\n",
            "dark\n",
            "gray\n",
            "primaries\n",
            "and\n",
            "rectricles\n",
            "====================================\n",
            "9\n",
            "this\n",
            "is\n",
            "slender\n",
            "grey\n",
            "and\n",
            "black\n",
            "bird\n",
            "with\n",
            "a\n",
            "slim\n",
            "and\n",
            "long\n",
            "bill\n",
            "and\n",
            "black\n",
            "with\n",
            "grey\n",
            "belly\n",
            "====================================\n",
            "10\n",
            "this\n",
            "bird\n",
            "has\n",
            "grey\n",
            "green\n",
            "and\n",
            "black\n",
            "secondaries\n",
            "a\n",
            "grey\n",
            "head\n",
            "white\n",
            "eye\n",
            "ring\n",
            "and\n",
            "yellow\n",
            "breast\n",
            "<end>\n",
            "====================================\n",
            "11\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "a\n",
            "long\n",
            "slim\n",
            "black\n",
            "beak\n",
            "yellow\n",
            "belly\n",
            "a\n",
            "black\n",
            "crown\n",
            "and\n",
            "white\n",
            "retrices\n",
            "<end>\n",
            "====================================\n",
            "12\n",
            "this\n",
            "very\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "and\n",
            "green\n",
            "back\n",
            "this\n",
            "a\n",
            "long\n",
            "thin\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "13\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "eye\n",
            "ring\n",
            "belly\n",
            "and\n",
            "vent\n",
            "and\n",
            "speckled\n",
            "black\n",
            "and\n",
            "white\n",
            "secondaries\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "14\n",
            "this\n",
            "bird\n",
            "has\n",
            "black\n",
            "wings\n",
            "breast\n",
            "tail\n",
            "and\n",
            "feet\n",
            "and\n",
            "red\n",
            "white\n",
            "and\n",
            "black\n",
            "head\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "15\n",
            "this\n",
            "small\n",
            "brown\n",
            "green\n",
            "bird\n",
            "has\n",
            "dark\n",
            "round\n",
            "eyes\n",
            "and\n",
            "features\n",
            "a\n",
            "small\n",
            "thin\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "16\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "black\n",
            "overall\n",
            "body\n",
            "color\n",
            "even\n",
            "up\n",
            "to\n",
            "its\n",
            "feet\n",
            "and\n",
            "tarsus\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "17\n",
            "this\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "yellow\n",
            "body\n",
            "with\n",
            "a\n",
            "black\n",
            "breast\n",
            "and\n",
            "blue\n",
            "cheek\n",
            "patch\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "18\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "black\n",
            "secondaries\n",
            "white\n",
            "wing\n",
            "bars\n",
            "and\n",
            "a\n",
            "black\n",
            "cheek\n",
            "patch\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "19\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "black\n",
            "head\n",
            "a\n",
            "long\n",
            "black\n",
            "bill\n",
            "and\n",
            "a\n",
            "large\n",
            "wingspan\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "20\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "very\n",
            "large\n",
            "black\n",
            "and\n",
            "white\n",
            "fully\n",
            "head\n",
            "and\n",
            "orange\n",
            "eyes\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "21\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "brown\n",
            "crown\n",
            "as\n",
            "well\n",
            "as\n",
            "a\n",
            "brown\n",
            "and\n",
            "white\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "22\n",
            "this\n",
            "puffy\n",
            "bird\n",
            "has\n",
            "a\n",
            "patterned\n",
            "golden\n",
            "brown\n",
            "and\n",
            "white\n",
            "breast\n",
            "and\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "23\n",
            "the\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "brown\n",
            "body\n",
            "with\n",
            "black\n",
            "markings\n",
            "on\n",
            "its\n",
            "crown\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "24\n",
            "this\n",
            "bird\n",
            "is\n",
            "white\n",
            "black\n",
            "and\n",
            "has\n",
            "an\n",
            "orange\n",
            "spot\n",
            "on\n",
            "its\n",
            "side\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "25\n",
            "this\n",
            "bird\n",
            "is\n",
            "yellow\n",
            "white\n",
            "and\n",
            "brown\n",
            "in\n",
            "color\n",
            "with\n",
            "a\n",
            "black\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "26\n",
            "this\n",
            "small\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "gray\n",
            "back\n",
            "and\n",
            "pointed\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "27\n",
            "this\n",
            "blue\n",
            "bird\n",
            "has\n",
            "a\n",
            "yellow\n",
            "eye\n",
            "black\n",
            "superciliary\n",
            "and\n",
            "black\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "28\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "brown\n",
            "and\n",
            "has\n",
            "a\n",
            "grey\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "29\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "brown\n",
            "and\n",
            "has\n",
            "a\n",
            "yellow\n",
            "throat\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "30\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "yellow\n",
            "body\n",
            "black\n",
            "head\n",
            "and\n",
            "short\n",
            "sharp\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "31\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "grey\n",
            "and\n",
            "has\n",
            "a\n",
            "yellow\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "32\n",
            "this\n",
            "bird\n",
            "is\n",
            "white\n",
            "and\n",
            "brown\n",
            "in\n",
            "color\n",
            "with\n",
            "a\n",
            "black\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "33\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "black\n",
            "and\n",
            "has\n",
            "an\n",
            "orange\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "34\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "long\n",
            "and\n",
            "pointy\n",
            "beak\n",
            "with\n",
            "a\n",
            "red\n",
            "nape\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "35\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "brown\n",
            "and\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "36\n",
            "a\n",
            "large\n",
            "white\n",
            "seabird\n",
            "with\n",
            "black\n",
            "secondaries\n",
            "and\n",
            "a\n",
            "sharp\n",
            "yellow\n",
            "bill\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "37\n",
            "a\n",
            "brightly\n",
            "colored\n",
            "red\n",
            "bird\n",
            "with\n",
            "a\n",
            "black\n",
            "tail\n",
            "and\n",
            "black\n",
            "wings\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "38\n",
            "the\n",
            "bird\n",
            "is\n",
            "dark\n",
            "black\n",
            "in\n",
            "color\n",
            "and\n",
            "has\n",
            "bright\n",
            "red\n",
            "eyes\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "39\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "white\n",
            "belly\n",
            "black\n",
            "wing\n",
            "and\n",
            "a\n",
            "grey\n",
            "head\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "40\n",
            "this\n",
            "bird\n",
            "has\n",
            "wings\n",
            "that\n",
            "are\n",
            "grey\n",
            "and\n",
            "has\n",
            "a\n",
            "yellow\n",
            "belly\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "41\n",
            "a\n",
            "small\n",
            "bird\n",
            "with\n",
            "bright\n",
            "multi\n",
            "coloring\n",
            "and\n",
            "red\n",
            "around\n",
            "eyes\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "42\n",
            "a\n",
            "small\n",
            "yellow\n",
            "bird\n",
            "with\n",
            "gray\n",
            "wings\n",
            "and\n",
            "a\n",
            "tan\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "43\n",
            "a\n",
            "brown\n",
            "bird\n",
            "with\n",
            "a\n",
            "black\n",
            "crown\n",
            "and\n",
            "a\n",
            "yellow\n",
            "throat\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "44\n",
            "this\n",
            "bird\n",
            "has\n",
            "a\n",
            "orange\n",
            "head\n",
            "a\n",
            "black\n",
            "and\n",
            "white\n",
            "body\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "45\n",
            "the\n",
            "bird\n",
            "is\n",
            "brown\n",
            "with\n",
            "a\n",
            "yellow\n",
            "eyering\n",
            "and\n",
            "small\n",
            "tarsals\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "46\n",
            "small\n",
            "white\n",
            "and\n",
            "mahogany\n",
            "bird\n",
            "with\n",
            "yellow\n",
            "throat\n",
            "and\n",
            "eyebrows\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "47\n",
            "the\n",
            "bird\n",
            "has\n",
            "a\n",
            "small\n",
            "black\n",
            "eyering\n",
            "and\n",
            "small\n",
            "bill\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n",
            "48\n",
            "a\n",
            "dark\n",
            "black\n",
            "bird\n",
            "with\n",
            "a\n",
            "black\n",
            "pointed\n",
            "beak\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "<end>\n",
            "====================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X1Ojwf0MqoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "dcc42afc-3e86-4180-d693-141d0fbb4cab"
      },
      "source": [
        "filepath = os.path.join('/content', 'captions.pickle')\n",
        "with open(filepath, 'rb') as f:\n",
        "    x = pickle.load(f)\n",
        "    train_captions, test_captions = x[0], x[1]\n",
        "    ixtoword, wordtoix = x[2], x[3]\n",
        "    del x\n",
        "    n_words = len(ixtoword)\n",
        "    print('Load from: ', filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load from:  /content/captions.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcrt9BZrFWPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fb75f452-0e27-41fb-c0b7-de2a82fc3039"
      },
      "source": [
        "x = nn.Upsample(size=(299, 299), mode='bilinear')(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBI6bAeA-7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step, data in enumerate(dataloader, 0):\n",
        "  print('step ===> ', step)\n",
        "  imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
        "  print('size: ',imgs[-1].size(), 'first number: ', imgs[-1][0][0][0][0])\n",
        "  # words_features: batch_size x nef x 17 x 17\n",
        "  # sent_code: batch_size x nef\n",
        "  words_features, sent_code = image_encoder(imgs[-1])\n",
        "  print ('words_features', words_features.size())\n",
        "  print ('sent_code', sent_code.size())\n",
        "\n",
        "  print('================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcPYjphKieqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c87a8dae-d1f3-4a06-9db2-ee2231c965fd"
      },
      "source": [
        "import pickle\n",
        "filepath = '/content/filenames.pickle'\n",
        "with open(filepath, 'rb') as f:\n",
        "  filenames = pickle.load(f)\n",
        "print('Load filenames from: %s (%d)' % (filepath, len(filenames)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load filenames from: /content/filenames.pickle (8855)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlcltDDz6GGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "600339ad-1a4a-4993-ed66-d59114a49783"
      },
      "source": [
        "len(filenames)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA7sAaTw4aQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_id = []\n",
        "for name in filenames:\n",
        "  labels_id.append(int(name[:3]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGUgGb5q6w0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "150762e2-baca-4dd1-dadd-d538f016bbe5"
      },
      "source": [
        "labels_id.index(2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY2hYKJt475j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for id, name in zip(labels_id, filenames):\n",
        "  if id < 100:\n",
        "    print(id, '--', name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQjN2s_FjMFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b5b1c26e-75aa-4095-fb07-d6613b0f4b3f"
      },
      "source": [
        "int(filenames[0][:3])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IneuOKMd53GI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/text.zip"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxSYzVY1i-YC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/__MACOSX/"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUz51myOh8RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKZBzQGSlWmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8cDTBH1mn1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(list(zip(labels, labels_id, sentences)), columns =['Labels', 'ID', 'Sentences']) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeA6wQSX6fw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "60bf9a91-c7ad-4eb8-993d-3d4e9185ded1"
      },
      "source": [
        "df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>ID</th>\n",
              "      <th>Sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>a bird with a very long wing span and a long p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>the long-beaked bird has a white body with lon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>this is a white bird with brown wings and a la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>this large bird has long bill, a white breast,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>002.Laysan_Albatross/Laysan_Albatross_0002_1027</td>\n",
              "      <td>2</td>\n",
              "      <td>bird has an extremely long wingspan with a dar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8850</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>the crown of the bird is brown the body is bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8851</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>a dark brown bird with black spots, light brow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8852</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>this bird is brown and black in color with a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8853</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>the bird has an oval shaped, small eye and the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8854</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986</td>\n",
              "      <td>200</td>\n",
              "      <td>this bird has a flat head an extremely small b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8855 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Labels  ...                                          Sentences\n",
              "0       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  a bird with a very long wing span and a long p...\n",
              "1       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  the long-beaked bird has a white body with lon...\n",
              "2       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  this is a white bird with brown wings and a la...\n",
              "3       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  this large bird has long bill, a white breast,...\n",
              "4       002.Laysan_Albatross/Laysan_Albatross_0002_1027  ...  bird has an extremely long wingspan with a dar...\n",
              "...                                                 ...  ...                                                ...\n",
              "8850  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  the crown of the bird is brown the body is bro...\n",
              "8851  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  a dark brown bird with black spots, light brow...\n",
              "8852  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  this bird is brown and black in color with a s...\n",
              "8853  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  the bird has an oval shaped, small eye and the...\n",
              "8854  022.Chuck_will_Widow/Chuck_Will_Widow_0006_796986  ...  this bird has a flat head an extremely small b...\n",
              "\n",
              "[8855 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzBVsgyNnEPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('CUB_captions.csv') "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1kBfX7skRoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "all_captions = []\n",
        "for i in range(len(filenames)):\n",
        "    cap_path = '/content/text/%s.txt' % ( filenames[i])\n",
        "    with open(cap_path, \"r\") as f:\n",
        "        captions = f.read().split('\\n')\n",
        "        if i < 8900:\n",
        "          print(i)\n",
        "          for j in captions :\n",
        "            if j=='':\n",
        "              continue\n",
        "            sentences.append(j)\n",
        "            labels.append(filenames[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "49beea18-613d-4d89-85b1-9fdb33866752"
      },
      "source": [
        "all_captions = []\n",
        "for i in range(len(filenames)):\n",
        "    cap_path = '/content/text/%s.txt' % ( filenames[i])\n",
        "    with open(cap_path, \"r\") as f:\n",
        "        captions = f.read().decode('utf8').split('\\n')\n",
        "        cnt = 0\n",
        "        for cap in captions:\n",
        "            if len(cap) == 0:\n",
        "                continue\n",
        "            cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "            # picks out sequences of alphanumeric characters as tokens\n",
        "            # and drops everything else\n",
        "            tokenizer = RegexpTokenizer(r'\\w+')\n",
        "            tokens = tokenizer.tokenize(cap.lower())\n",
        "            # print('tokens', tokens)\n",
        "            if len(tokens) == 0:\n",
        "                print('cap', cap)\n",
        "                continue\n",
        "\n",
        "            tokens_new = []\n",
        "            for t in tokens:\n",
        "                t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                if len(t) > 0:\n",
        "                    tokens_new.append(t)\n",
        "            all_captions.append(tokens_new)\n",
        "            cnt += 1\n",
        "            if cnt == self.embeddings_num:\n",
        "                break\n",
        "        if cnt < self.embeddings_num:\n",
        "            print('ERROR: the captions for %s less than %d'\n",
        "                  % (filenames[i], cnt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPsXrpHhdgM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QjAGCOOqNtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtp95TB6qWdC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "75c7fec6-da5e-4baa-8282-74e2ae144ee6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/CUB_captions.csv\", header=None, names=['Label',  'ID', 'Sentences'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,856\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>ID</th>\n",
              "      <th>Sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2605.0</th>\n",
              "      <td>010.Red_winged_Blackbird/Red_Winged_Blackbird_...</td>\n",
              "      <td>64</td>\n",
              "      <td>this bird is black with red and has a long, po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5003.0</th>\n",
              "      <td>015.Lazuli_Bunting/Lazuli_Bunting_0082_15047</td>\n",
              "      <td>122</td>\n",
              "      <td>a small bird with a blue head and yellow belly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3897.0</th>\n",
              "      <td>012.Yellow_headed_Blackbird/Yellow_Headed_Blac...</td>\n",
              "      <td>93</td>\n",
              "      <td>a small bird with a yellow head and breast alo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5931.0</th>\n",
              "      <td>017.Cardinal/Cardinal_0029_17297</td>\n",
              "      <td>140</td>\n",
              "      <td>small bird with upright reddish feathers in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7646.0</th>\n",
              "      <td>020.Yellow_breasted_Chat/Yellow_Breasted_Chat_...</td>\n",
              "      <td>174</td>\n",
              "      <td>this bird has a white belly, a yellow breast a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492.0</th>\n",
              "      <td>005.Crested_Auklet/Crested_Auklet_0076_785252</td>\n",
              "      <td>42</td>\n",
              "      <td>a black body, white eye with stripe next to it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257.0</th>\n",
              "      <td>005.Crested_Auklet/Crested_Auklet_0044_1825</td>\n",
              "      <td>32</td>\n",
              "      <td>this bird is grey with blue and has a very sho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218.0</th>\n",
              "      <td>005.Crested_Auklet/Crested_Auklet_0018_1817</td>\n",
              "      <td>30</td>\n",
              "      <td>this bird is gray and brown in color, and has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421.0</th>\n",
              "      <td>010.Red_winged_Blackbird/Red_Winged_Blackbird_...</td>\n",
              "      <td>61</td>\n",
              "      <td>small black bird with a short black beak and b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8703.0</th>\n",
              "      <td>022.Chuck_will_Widow/Chuck_Will_Widow_0047_796971</td>\n",
              "      <td>198</td>\n",
              "      <td>a stout, brown bird speckled with white with e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Label  ...                                          Sentences\n",
              "2605.0  010.Red_winged_Blackbird/Red_Winged_Blackbird_...  ...  this bird is black with red and has a long, po...\n",
              "5003.0       015.Lazuli_Bunting/Lazuli_Bunting_0082_15047  ...  a small bird with a blue head and yellow belly...\n",
              "3897.0  012.Yellow_headed_Blackbird/Yellow_Headed_Blac...  ...  a small bird with a yellow head and breast alo...\n",
              "5931.0                   017.Cardinal/Cardinal_0029_17297  ...  small bird with upright reddish feathers in th...\n",
              "7646.0  020.Yellow_breasted_Chat/Yellow_Breasted_Chat_...  ...  this bird has a white belly, a yellow breast a...\n",
              "1492.0      005.Crested_Auklet/Crested_Auklet_0076_785252  ...  a black body, white eye with stripe next to it...\n",
              "1257.0        005.Crested_Auklet/Crested_Auklet_0044_1825  ...  this bird is grey with blue and has a very sho...\n",
              "1218.0        005.Crested_Auklet/Crested_Auklet_0018_1817  ...  this bird is gray and brown in color, and has ...\n",
              "2421.0  010.Red_winged_Blackbird/Red_Winged_Blackbird_...  ...  small black bird with a short black beak and b...\n",
              "8703.0  022.Chuck_will_Widow/Chuck_Will_Widow_0047_796971  ...  a stout, brown bird speckled with white with e...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ZKbuVkqwdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "import numpy as np\n",
        "\n",
        "sentences = df.Sentences.values\n",
        "labels_text = df.Label.values\n",
        "labels = df.ID.values\n",
        "\n",
        "labels = np.delete(labels, 0)\n",
        "labels_text = np.delete(labels_text, 0)\n",
        "sentences = np.delete(sentences, 0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT4m4nzZrWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = labels.astype(int)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sASty7Vl8zjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ff6a67a5-3554-4c74-fdc6-52c81fc3f2dc"
      },
      "source": [
        "labels"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  2,   2,   2,  ..., 200, 200, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoScn6RP8u8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ef8bd9e6-981a-492e-c5de-6fda18a8ede0"
      },
      "source": [
        "import torch\n",
        "print (labels)\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  2   2   2 ... 200 200 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC8iMadFrbfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWREbAAF8Uvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYL7RPBbsw9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2ac9afc6-b5a8-4142-8f5e-c59835748bbc"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  a bird with a very long wing span and a long pointed beak.\n",
            "Tokenized:  ['a', 'bird', 'with', 'a', 'very', 'long', 'wing', 'span', 'and', 'a', 'long', 'pointed', 'beak', '.']\n",
            "Token IDs:  [1037, 4743, 2007, 1037, 2200, 2146, 3358, 8487, 1998, 1037, 2146, 4197, 23525, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ufKmKoRs2Nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "4a11a83a-0562-46bd-b3a8-63831d6b4bb3"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    if  len(input_ids) == 80:\n",
        "      print(sent)\n",
        "      print(len(sent.split()))\n",
        "      print(sentences.index(sent))\n",
        "\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1914edf96f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# For every sentence...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Tokenize the text and add `[CLS]` and `[SEP]` tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZhTczPhtTqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atwal = []\n",
        "for i in sentences:\n",
        "  atwal.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZPTynGpt45_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atwal.sort(reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnIqqmdtu2Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atwal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUizQ2wrv-3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation=True\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dqg9xc7x5Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}