{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextEncoderCubAttn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AD086bErzXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "138acf11-267b-4489-84bc-1651fd960902"
      },
      "source": [
        "import os\n",
        "!rm -r sample_data\n",
        "\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Dwonload files config.py, utils.py, datasets.py, pretrain_DAMSM.py\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "#Move code files to their Locations\n",
        "!mv /content/CUB-Attn-GAN/theCode/config.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/CUB-Attn-GAN/theCode/utils.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/CUB-Attn-GAN/theCode/pretrain_DAMSM.py /content/AttnGAN/code/\n",
        "!mv /content/CUB-Attn-GAN/theCode/datasets.py /content/AttnGAN/code/\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "print(os.getcwd())\n",
        "\n",
        "#Downloads birds.zip (6.19M) , Extract it , and remove unnesscery files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "print(os.getcwd())\n",
        "\n",
        "#Download CUB_200_2011.tgz (1.07G), , Extract it , and remove unnesscery files\n",
        "#!wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\" -O CUB_200_2011.tgz && rm -rf /tmp/cookies.txt\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "#Changing Working dirctory to DAMSMencoders\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "print(os.getcwd())\n",
        "\n",
        "#Download birds.zip (87.15M), , Extract it , and remove unnesscery files\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V\" -O birds.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "\n",
        "#Changing Working dirctory to models\n",
        "os.chdir('/content/AttnGAN/models/')\n",
        "print(os.getcwd())\n",
        "\n",
        "#Download bird_AttnDCGAN2.pth (25.46M).\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19TG0JUoXurxsmZLaJ82Yo6O0UJ6aDBpg' -O bird_AttnDCGAN2.pth\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "print(os.getcwd())\n",
        "\n",
        "#Download Pillow.rar (251.75K), , Extract it , and remove unnesscery files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AttnGAN'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 36.76 MiB | 9.61 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "Cloning into 'CUB-Attn-GAN'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Total 550 (delta 0), reused 0 (delta 0), pack-reused 550\u001b[K\n",
            "Receiving objects: 100% (550/550), 465.18 MiB | 14.69 MiB/s, done.\n",
            "Resolving deltas: 100% (300/300), done.\n",
            "Checking out files: 100% (84/84), done.\n",
            "/content/AttnGAN/data\n",
            "--2020-07-14 19:00:28--  https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.189.138, 64.233.189.139, 64.233.189.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.189.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fh65teprooqdhip4re3vvvc3jhtojjmj/1594753200000/09657060183789739732/*/1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-14 19:00:58--  https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fh65teprooqdhip4re3vvvc3jhtojjmj/1594753200000/09657060183789739732/*/1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ?e=download\n",
            "Resolving doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘birds.zip’\n",
            "\n",
            "birds.zip               [  <=>               ]   6.19M  21.5MB/s    in 0.3s    \n",
            "\n",
            "2020-07-14 19:00:59 (21.5 MB/s) - ‘birds.zip’ saved [6488322]\n",
            "\n",
            "/content/AttnGAN/data/birds\n",
            "--2020-07-14 19:01:06--  https://docs.google.com/uc?export=download&confirm=BeoP&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.204.102, 74.125.204.138, 74.125.204.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-bs-docs.googleusercontent.com/docs/securesc/91qo4g6fgj3sqs7lrakaav4dl2aj0v1s/djuiaju9rip6hlitbufg3nb4phqqo2qr/1594753200000/15424859768005087218/16633688501204801910Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download [following]\n",
            "--2020-07-14 19:01:07--  https://doc-14-bs-docs.googleusercontent.com/docs/securesc/91qo4g6fgj3sqs7lrakaav4dl2aj0v1s/djuiaju9rip6hlitbufg3nb4phqqo2qr/1594753200000/15424859768005087218/16633688501204801910Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download\n",
            "Resolving doc-14-bs-docs.googleusercontent.com (doc-14-bs-docs.googleusercontent.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to doc-14-bs-docs.googleusercontent.com (doc-14-bs-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=u9r8bsa4vu8fi&continue=https://doc-14-bs-docs.googleusercontent.com/docs/securesc/91qo4g6fgj3sqs7lrakaav4dl2aj0v1s/djuiaju9rip6hlitbufg3nb4phqqo2qr/1594753200000/15424859768005087218/16633688501204801910Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e%3Ddownload&hash=cuk99t1hipned4hjg2vf65obgp8je4pj [following]\n",
            "--2020-07-14 19:01:07--  https://docs.google.com/nonceSigner?nonce=u9r8bsa4vu8fi&continue=https://doc-14-bs-docs.googleusercontent.com/docs/securesc/91qo4g6fgj3sqs7lrakaav4dl2aj0v1s/djuiaju9rip6hlitbufg3nb4phqqo2qr/1594753200000/15424859768005087218/16633688501204801910Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e%3Ddownload&hash=cuk99t1hipned4hjg2vf65obgp8je4pj\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-14-bs-docs.googleusercontent.com/docs/securesc/91qo4g6fgj3sqs7lrakaav4dl2aj0v1s/djuiaju9rip6hlitbufg3nb4phqqo2qr/1594753200000/15424859768005087218/16633688501204801910Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download&nonce=u9r8bsa4vu8fi&user=16633688501204801910Z&hash=vvsocq7t5k4o9ven5hd4l6e0t832dobs [following]\n",
            "--2020-07-14 19:01:08--  https://doc-14-bs-docs.googleusercontent.com/docs/securesc/91qo4g6fgj3sqs7lrakaav4dl2aj0v1s/djuiaju9rip6hlitbufg3nb4phqqo2qr/1594753200000/15424859768005087218/16633688501204801910Z/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45?e=download&nonce=u9r8bsa4vu8fi&user=16633688501204801910Z&hash=vvsocq7t5k4o9ven5hd4l6e0t832dobs\n",
            "Connecting to doc-14-bs-docs.googleusercontent.com (doc-14-bs-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gtar]\n",
            "Saving to: ‘CUB_200_2011.tgz’\n",
            "\n",
            "CUB_200_2011.tgz        [               <=>  ]   1.07G  55.0MB/s    in 18s     \n",
            "\n",
            "2020-07-14 19:01:27 (62.4 MB/s) - ‘CUB_200_2011.tgz’ saved [1150585339]\n",
            "\n",
            "/content/AttnGAN/DAMSMencoders\n",
            "--2020-07-14 19:01:45--  https://docs.google.com/uc?export=download&confirm=F9VB&id=1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.203.100, 74.125.203.138, 74.125.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-8g-docs.googleusercontent.com/docs/securesc/0hh68rop9lropq6jgdtnah28o2tb9dvb/ssiflljchbb7osutkrdog918qqqmum5i/1594753275000/09657060183789739732/10600744944795263513Z/1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V?e=download [following]\n",
            "--2020-07-14 19:01:46--  https://doc-0o-8g-docs.googleusercontent.com/docs/securesc/0hh68rop9lropq6jgdtnah28o2tb9dvb/ssiflljchbb7osutkrdog918qqqmum5i/1594753275000/09657060183789739732/10600744944795263513Z/1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V?e=download\n",
            "Resolving doc-0o-8g-docs.googleusercontent.com (doc-0o-8g-docs.googleusercontent.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to doc-0o-8g-docs.googleusercontent.com (doc-0o-8g-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=rlum3lk8oj1qa&continue=https://doc-0o-8g-docs.googleusercontent.com/docs/securesc/0hh68rop9lropq6jgdtnah28o2tb9dvb/ssiflljchbb7osutkrdog918qqqmum5i/1594753275000/09657060183789739732/10600744944795263513Z/1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V?e%3Ddownload&hash=fh2pd7gee24gb92bcqiqi1trq51oa4ao [following]\n",
            "--2020-07-14 19:01:46--  https://docs.google.com/nonceSigner?nonce=rlum3lk8oj1qa&continue=https://doc-0o-8g-docs.googleusercontent.com/docs/securesc/0hh68rop9lropq6jgdtnah28o2tb9dvb/ssiflljchbb7osutkrdog918qqqmum5i/1594753275000/09657060183789739732/10600744944795263513Z/1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V?e%3Ddownload&hash=fh2pd7gee24gb92bcqiqi1trq51oa4ao\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0o-8g-docs.googleusercontent.com/docs/securesc/0hh68rop9lropq6jgdtnah28o2tb9dvb/ssiflljchbb7osutkrdog918qqqmum5i/1594753275000/09657060183789739732/10600744944795263513Z/1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V?e=download&nonce=rlum3lk8oj1qa&user=10600744944795263513Z&hash=emc7k16hvo01l6974meh2med37u4ammc [following]\n",
            "--2020-07-14 19:01:47--  https://doc-0o-8g-docs.googleusercontent.com/docs/securesc/0hh68rop9lropq6jgdtnah28o2tb9dvb/ssiflljchbb7osutkrdog918qqqmum5i/1594753275000/09657060183789739732/10600744944795263513Z/1GNUKjVeyWYBJ8hEU-yrfYQpDOkxEyP3V?e=download&nonce=rlum3lk8oj1qa&user=10600744944795263513Z&hash=emc7k16hvo01l6974meh2med37u4ammc\n",
            "Connecting to doc-0o-8g-docs.googleusercontent.com (doc-0o-8g-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘birds.zip’\n",
            "\n",
            "birds.zip               [      <=>           ]  87.15M  59.4MB/s    in 1.5s    \n",
            "\n",
            "2020-07-14 19:01:49 (59.4 MB/s) - ‘birds.zip’ saved [91382224]\n",
            "\n",
            "/content/AttnGAN/models\n",
            "--2020-07-14 19:01:55--  https://docs.google.com/uc?export=download&id=19TG0JUoXurxsmZLaJ82Yo6O0UJ6aDBpg\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.203.113, 74.125.203.102, 74.125.203.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.203.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ao940l3gkfklaasfmgtu7otjcnsi80ol/1594753275000/09657060183789739732/*/19TG0JUoXurxsmZLaJ82Yo6O0UJ6aDBpg?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-14 19:01:57--  https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ao940l3gkfklaasfmgtu7otjcnsi80ol/1594753275000/09657060183789739732/*/19TG0JUoXurxsmZLaJ82Yo6O0UJ6aDBpg?e=download\n",
            "Resolving doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘bird_AttnDCGAN2.pth’\n",
            "\n",
            "bird_AttnDCGAN2.pth     [  <=>               ]  25.46M  54.5MB/s    in 0.5s    \n",
            "\n",
            "2020-07-14 19:01:58 (54.5 MB/s) - ‘bird_AttnDCGAN2.pth’ saved [26700350]\n",
            "\n",
            "/content/AttnGAN/code\n",
            "--2020-07-14 19:02:00--  https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.203.113, 74.125.203.138, 74.125.203.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.203.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk6agu54s04139lt7pe5iam8v7vd9urv/1594753275000/17309505201871794426/*/1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-07-14 19:02:01--  https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk6agu54s04139lt7pe5iam8v7vd9urv/1594753275000/17309505201871794426/*/1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111?e=download\n",
            "Resolving doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257793 (252K) [application/x-rar]\n",
            "Saving to: ‘Pillow.rar’\n",
            "\n",
            "Pillow.rar          100%[===================>] 251.75K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-07-14 19:02:02 (104 MB/s) - ‘Pillow.rar’ saved [257793/257793]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Pillow.rar\n",
            "\n",
            "Creating    Pillow                                                    OK\n",
            "Creating    Pillow/Tests                                              OK\n",
            "Creating    Pillow/Tests/fonts                                        OK\n",
            "Extracting  Pillow/Tests/fonts/FreeMono.ttf                              \b\b\b\b 12%\b\b\b\b 25%\b\b\b\b 38%\b\b\b\b 50%\b\b\b\b 63%\b\b\b\b 76%\b\b\b\b 88%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q_XfHl8xWXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "35b7fb7a-165e-4bc4-c8e8-904cb5a8d1b0"
      },
      "source": [
        "os.chdir('/content')\n",
        "!rm -r CUB-Attn-GAN\n",
        "\n",
        "#Dwonload files config.py, utils.py, datasets.py, pretrain_DAMSM.py\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "#Move code files to their Locations\n",
        "!mv /content/CUB-Attn-GAN/theCode/config.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/CUB-Attn-GAN/theCode/utils.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/CUB-Attn-GAN/theCode/pretrain_DAMSM.py /content/AttnGAN/code/\n",
        "!mv /content/CUB-Attn-GAN/theCode/datasets.py /content/AttnGAN/code/\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CUB-Attn-GAN'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Total 550 (delta 0), reused 0 (delta 0), pack-reused 550\u001b[K\n",
            "Receiving objects: 100% (550/550), 465.18 MiB | 14.54 MiB/s, done.\n",
            "Resolving deltas: 100% (300/300), done.\n",
            "Checking out files: 100% (84/84), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QApgRQ9DdBm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e52f553d-4796-40b3-85c2-d1fcc6760abd"
      },
      "source": [
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python pretrain_DAMSM.py --cfg cfg/DAMSM/bird.yml --gpu 0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'DAMSM',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 128,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 48,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '',\n",
            "           'NET_G': '',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 1.0},\n",
            "           'SNAPSHOT_INTERVAL': 50},\n",
            " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
            " 'WORKERS': 1}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "5450 10\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:00<00:00, 120MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "keyword |||||||||||||||||||||||||||||||\n",
            "Start_epoch :  0\n",
            "cfg.TRAIN.MAX_EPOCH :  600\n",
            "keyword |||||||||||||||||||||||||||||||\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "pretrain_DAMSM.py:103: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  cfg.TRAIN.RNN_GRAD_CLIP)\n",
            "s_total_loss0.item()/UPDATE_INTERVAL :  0.01954487681388855\n",
            "s_total_loss1.item()/UPDATE_INTERVAL :  0.019785275459289552\n",
            "w_total_loss0.item()/UPDATE_INTERVAL :  0.02698720693588257\n",
            "w_total_loss1.item()/UPDATE_INTERVAL :  0.021053106784820558\n",
            "| epoch   0 |     0/  184 batches | ms/batch 27.54 | s_loss  0.02  0.02 | w_loss  0.03  0.02\n",
            "bulding images\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  240.15968227386475\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "Traceback (most recent call last):\n",
            "  File \"pretrain_DAMSM.py\", line 304, in <module>\n",
            "    dataset.ixtoword, image_dir)\n",
            "  File \"pretrain_DAMSM.py\", line 151, in train\n",
            "    im.save(drivepath)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2099, in save\n",
            "    fp = builtins.open(filename, \"w+b\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/cubImage/attention_maps0.png'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gONfh1Fi2eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train()\n",
        "    rnn_model.train()\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # print('step', step) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, \\\n",
        "            class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x nef x 17 x 17\n",
        "        # sent_code: batch_size x nef\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "UPDATE_INTERVAL = 200\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % \\\n",
        "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
        "    batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(int(imsize * 76 / 64)), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters())\n",
        "    for v in image_encoder.parameters():\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train()\n",
        "    rnn_model.train()\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # print('step', step) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, \\\n",
        "            class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x nef x 17 x 17\n",
        "        # sent_code: batch_size x nef\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "UPDATE_INTERVAL = 200\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % \\\n",
        "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
        "    batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(int(imsize * 76 / 64)), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters())\n",
        "    for v in image_encoder.parameters():\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train()\n",
        "    rnn_model.train()\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # print('step', step) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, \\\n",
        "            class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x nef x 17 x 17\n",
        "        # sent_code: batch_size x nef\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "UPDATE_INTERVAL = 200\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % \\\n",
        "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
        "    batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(int(imsize * 76 / 64)), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters())\n",
        "    for v in image_encoder.parameters():\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train()\n",
        "    rnn_model.train()\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # print('step', step) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, \\\n",
        "            class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x nef x 17 x 17\n",
        "        # sent_code: batch_size x nef\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "UPDATE_INTERVAL = 200\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % \\\n",
        "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
        "    batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(int(imsize * 76 / 64)), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters())\n",
        "    for v in image_encoder.parameters():\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = \\\n",
        "        torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox()\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, \\\n",
        "            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "        self.number_example = len(self.filenames)\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'\n",
        "                          % (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                 nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                               self.nlayers, batch_first=True,\n",
        "                               dropout=self.drop_prob,\n",
        "                               bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                              self.nlayers, batch_first=True,\n",
        "                              dropout=self.drop_prob,\n",
        "                              bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                       bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels,\n",
        "               cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "    for i in range(batch_size):\n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
        "            labels, optimizer, epoch, ixtoword, image_dir):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train()\n",
        "    rnn_model.train()\n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        # print('step', step) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, \\\n",
        "            class_ids, keys = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x nef x 17 x 17\n",
        "        # sent_code: batch_size x nef\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
        "                                                    cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
        "                                        cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, \\\n",
        "                class_ids, keys = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "UPDATE_INTERVAL = 200\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % \\\n",
        "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
        "    batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(int(imsize * 76 / 64)), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
        "    para = list(text_encoder.parameters())\n",
        "    for v in image_encoder.parameters():\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder,\n",
        "                            batch_size, labels, optimizer, epoch,\n",
        "                            dataset.ixtoword, image_dir)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
        "                                            text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss '\n",
        "                        '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
        "                        .format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(),\n",
        "                            '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(),\n",
        "                            '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "                \n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP35RynGxsKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}