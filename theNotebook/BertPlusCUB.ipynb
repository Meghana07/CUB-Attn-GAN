{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertPlusCUB.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "190327803dba4965905cf88adedf3283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d1c2361e7c74b80b264f7835ebad887",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9909724ec9be42088cebf774a5c4b563",
              "IPY_MODEL_f2b5e3eca82e4394b6600528eeda07dd"
            ]
          }
        },
        "7d1c2361e7c74b80b264f7835ebad887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9909724ec9be42088cebf774a5c4b563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_430f05d8c8994d248b503d251901fee6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a49b59cec5944af9e8a6274ca1a78d0"
          }
        },
        "f2b5e3eca82e4394b6600528eeda07dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de9771f9d52340e9be4f4ac21f32310e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 704kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09c8944ff1cf44a1958c31c1f5dc4e5a"
          }
        },
        "430f05d8c8994d248b503d251901fee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a49b59cec5944af9e8a6274ca1a78d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de9771f9d52340e9be4f4ac21f32310e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09c8944ff1cf44a1958c31c1f5dc4e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ecb79c3b577c47df84fe30ef12adda64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b5d66bf5423d4e93944c882911c9ec58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_93d46d39c387475a923ec0cbf03966ed",
              "IPY_MODEL_2f4809e1004d41f3857a6ad21d72b1a5"
            ]
          }
        },
        "b5d66bf5423d4e93944c882911c9ec58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93d46d39c387475a923ec0cbf03966ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_544aff2f8f124a45bd243d5ff7cb8a5a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb3fa6c92e8a480b8410c05f899a6a92"
          }
        },
        "2f4809e1004d41f3857a6ad21d72b1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1f74aeb478b44620a696682ed586a73f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 3.21kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3708fbc2f9f049e28065df3d0f72d65c"
          }
        },
        "544aff2f8f124a45bd243d5ff7cb8a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb3fa6c92e8a480b8410c05f899a6a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f74aeb478b44620a696682ed586a73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3708fbc2f9f049e28065df3d0f72d65c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af956aac7fca4e53bbf0d0a39ef8873d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_79d6e3a33910469dbaa92ce658b5e46b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b158fa5e20ed4c42a6a461b53fcf9c93",
              "IPY_MODEL_0c322d22899741e5ab65d010d32bac3f"
            ]
          }
        },
        "79d6e3a33910469dbaa92ce658b5e46b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b158fa5e20ed4c42a6a461b53fcf9c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5dae5867589842deb825449082e6a237",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d213177d12bc4f7ca79a3aef4d9628a5"
          }
        },
        "0c322d22899741e5ab65d010d32bac3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f3fe60e2b6045a38ad17b28de9b972e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:23&lt;00:00, 18.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47eaa5db3c1642f58d08f5dcd3a1963c"
          }
        },
        "5dae5867589842deb825449082e6a237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d213177d12bc4f7ca79a3aef4d9628a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f3fe60e2b6045a38ad17b28de9b972e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47eaa5db3c1642f58d08f5dcd3a1963c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Iz9K9LIr7Nv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "f22f05f4-07b4-4b37-fe3f-64868ef22c8a"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "!rm -r sample_data\n",
        "\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\" -O CUB_200_2011.tgz && rm -rf /tmp/cookies.txt\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AttnGAN'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvrG7u6Rsxe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5e99507e-619b-478c-aef5-7b70ee674b35"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import datetime\n",
        "import dateutil.tz\n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from copy import deepcopy\n",
        "import skimage.transform\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "__C = edict()\n",
        "cfg = __C\n",
        "__C.DATASET_NAME = 'birds'\n",
        "__C.CONFIG_NAME = 'DAMSM'\n",
        "__C.DATA_DIR = '../data/birds'\n",
        "__C.GPU_ID = 0\n",
        "__C.CUDA = True\n",
        "__C.WORKERS = 1\n",
        "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
        "__C.B_VALIDATION = False\n",
        "\n",
        "__C.TREE = edict()\n",
        "__C.TREE.BRANCH_NUM = 1\n",
        "__C.TREE.BASE_SIZE = 299\n",
        "\n",
        "# Training options\n",
        "__C.TRAIN = edict()\n",
        "__C.TRAIN.BATCH_SIZE = 48\n",
        "__C.TRAIN.MAX_EPOCH = 600\n",
        "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
        "__C.TRAIN.DISCRIMINATOR_LR = 0.0002\n",
        "__C.TRAIN.GENERATOR_LR = 0.0002\n",
        "__C.TRAIN.ENCODER_LR = 0.002\n",
        "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
        "__C.TRAIN.FLAG = True\n",
        "__C.TRAIN.NET_E = ''\n",
        "__C.TRAIN.NET_G = ''\n",
        "__C.TRAIN.B_NET_D = True\n",
        "__C.TRAIN.SMOOTH = edict()\n",
        "__C.TRAIN.SMOOTH.GAMMA1 = 4.0\n",
        "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
        "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
        "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
        "\n",
        "# Modal options\n",
        "__C.GAN = edict()\n",
        "__C.GAN.DF_DIM = 64\n",
        "__C.GAN.GF_DIM = 128\n",
        "__C.GAN.Z_DIM = 100\n",
        "__C.GAN.CONDITION_DIM = 100\n",
        "__C.GAN.R_NUM = 2\n",
        "__C.GAN.B_ATTENTION = True\n",
        "__C.GAN.B_DCGAN = False\n",
        "\n",
        "__C.TEXT = edict()\n",
        "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
        "__C.TEXT.EMBEDDING_DIM = 256\n",
        "__C.TEXT.WORDS_NUM = 18\n",
        "\n",
        "\n",
        "\n",
        "def get_imgs(img_path, imsize, bbox=None,\n",
        "                transform=None, normalize=None):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - r)\n",
        "        y2 = np.minimum(height, center_y + r)\n",
        "        x1 = np.maximum(0, center_x - r)\n",
        "        x2 = np.minimum(width, center_x + r)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img)\n",
        "\n",
        "    ret = []\n",
        "    if cfg.GAN.B_DCGAN:\n",
        "        ret = [normalize(img)]\n",
        "    else:\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            # print(imsize[i])\n",
        "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
        "                re_img = transforms.Scale(imsize[i])(img)\n",
        "            else:\n",
        "                re_img = img\n",
        "            ret.append(normalize(re_img))\n",
        "\n",
        "    return ret\n",
        "\n",
        "def prepare_data(data):\n",
        "    imgs, captions, captions_lens, class_ids, keys, input_ids, segments_ids = data\n",
        "\n",
        "    # sort data by the length in a decreasing order !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!MARKER!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = imgs[i][sorted_cap_indices]\n",
        "        if cfg.CUDA:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    captions = captions[sorted_cap_indices].squeeze()\n",
        "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
        "    # sent_indices = sent_indices[sorted_cap_indices]\n",
        "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
        "    # print('keys', type(keys), keys[-1])  # list\n",
        "    if cfg.CUDA:\n",
        "        captions = Variable(captions).cuda()\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n",
        "    else:\n",
        "        captions = Variable(captions)\n",
        "        sorted_cap_lens = Variable(sorted_cap_lens)\n",
        "\n",
        "    return [real_imgs, captions, sorted_cap_lens,\n",
        "            class_ids, keys, input_ids, segments_ids]\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def build_super_images(real_imgs, captions, ixtoword,\n",
        "                        attn_maps, att_sze, lr_imgs=None,\n",
        "                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
        "                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
        "    \n",
        "    \n",
        "    COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
        "                2:[70, 70, 70],  3:[102,102,156],\n",
        "                4:[190,153,153], 5:[153,153,153],\n",
        "                6:[250,170, 30], 7:[220, 220, 0],\n",
        "                8:[107,142, 35], 9:[152,251,152],\n",
        "                10:[70,130,180], 11:[220,20, 60],\n",
        "                12:[255, 0, 0],  13:[0, 0, 142],\n",
        "                14:[119,11, 32], 15:[0, 60,100],\n",
        "                16:[0, 80, 100], 17:[0, 0, 230],\n",
        "                18:[0,  0, 70],  19:[0, 0,  0]}\n",
        "    FONT_MAX = 50\n",
        "\n",
        "    \n",
        "    build_super_images_start_time = time.time()\n",
        "    nvis = 8\n",
        "    real_imgs = real_imgs[:nvis]\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = lr_imgs[:nvis]\n",
        "    if att_sze == 17:\n",
        "        vis_size = att_sze * 16\n",
        "    else:\n",
        "        vis_size = real_imgs.size(2)\n",
        "\n",
        "    text_convas = \\\n",
        "        np.ones([batch_size * FONT_MAX,\n",
        "                 (max_word_num + 2) * (vis_size + 2), 3],\n",
        "                dtype=np.uint8)\n",
        "\n",
        "\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"max_word_num : \" , max_word_num)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    for i in range(max_word_num):\n",
        "        istart = (i + 2) * (vis_size + 2)\n",
        "        iend = (i + 3) * (vis_size + 2)\n",
        "        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
        "\n",
        "\n",
        "    real_imgs = \\\n",
        "        nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
        "    # [-1, 1] --> [0, 1]\n",
        "    real_imgs.add_(1).div_(2).mul_(255)\n",
        "    real_imgs = real_imgs.data.numpy()\n",
        "    # b x c x h x w --> b x h x w x c\n",
        "    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
        "    pad_sze = real_imgs.shape\n",
        "    middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
        "    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
        "    if lr_imgs is not None:\n",
        "        lr_imgs = \\\n",
        "            nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
        "        # [-1, 1] --> [0, 1]\n",
        "        lr_imgs.add_(1).div_(2).mul_(255)\n",
        "        lr_imgs = lr_imgs.data.numpy()\n",
        "        # b x c x h x w --> b x h x w x c\n",
        "        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
        "\n",
        "    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
        "    seq_len = max_word_num\n",
        "    img_set = []\n",
        "    num = nvis  # len(attn_maps)\n",
        "\n",
        "    text_map, sentences = \\\n",
        "        drawCaption(text_convas, captions, ixtoword, vis_size)\n",
        "    text_map = np.asarray(text_map).astype(np.uint8)\n",
        "\n",
        "    bUpdate = 1\n",
        "    for i in range(num):\n",
        "        #print (\"loop \" , i ,\" of \" , num == 8)\n",
        "        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
        "        # --> 1 x 1 x 17 x 17\n",
        "        attn_max = attn.max(dim=1, keepdim=True)\n",
        "        attn = torch.cat([attn_max[0], attn], 1)\n",
        "        #\n",
        "        attn = attn.view(-1, 1, att_sze, att_sze)\n",
        "        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
        "        # n x c x h x w --> n x h x w x c\n",
        "        attn = np.transpose(attn, (0, 2, 3, 1))\n",
        "        num_attn = attn.shape[0]\n",
        "        #\n",
        "        img = real_imgs[i]\n",
        "        if lr_imgs is None:\n",
        "            lrI = img\n",
        "        else:\n",
        "            lrI = lr_imgs[i]\n",
        "        \n",
        "        row = [lrI, middle_pad]\n",
        "        #print(\"rowwwwwwwwwwwwwwwww : \", row)\n",
        "        row_merge = [img, middle_pad]\n",
        "        row_beforeNorm = []\n",
        "        minVglobal, maxVglobal = 1, 0\n",
        "        for j in range(num_attn):\n",
        "            #print (\"looop \" , j , \" of \" , seq_len+1)\n",
        "            one_map = attn[j]\n",
        "            #print(\"First one map : \" , one_map.shape)\n",
        "            #print(\"attn.shape : \" , attn.shape)\n",
        "\n",
        "            \n",
        "            # print(\"if (vis_size // att_sze) > 1: \" ,  (vis_size // att_sze) > 1)\n",
        "            # print(\"vis_size : \" , vis_size)\n",
        "            # print(\"att_sze : \" , att_sze)\n",
        "            # print(\"vis_size//att_sze : \" , vis_size//att_sze)\n",
        "            \n",
        "            if (vis_size // att_sze) > 1:\n",
        "                one_map = \\\n",
        "                    skimage.transform.pyramid_expand(one_map, sigma=20,\n",
        "                                                     upscale=vis_size // att_sze)\n",
        "            #    print(\"one_map in if : \" , one_map.shape)\n",
        "\n",
        "            \n",
        "            row_beforeNorm.append(one_map)\n",
        "            #print(\"row_beforeNorm.append(one_map)\" ,len(row_beforeNorm))\n",
        "            minV = one_map.min()\n",
        "            maxV = one_map.max()\n",
        "            if minVglobal > minV:\n",
        "                minVglobal = minV\n",
        "            if maxVglobal < maxV:\n",
        "                maxVglobal = maxV\n",
        "            #print(\"seq_len : \" , seq_len)\n",
        "        for j in range(seq_len + 1):\n",
        "            #print (\"loooop \" , j , \" of \" , seq_len+1)\n",
        "            \n",
        "            if j < num_attn:\n",
        "                one_map = row_beforeNorm[j]\n",
        "                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
        "                one_map *= 255\n",
        "                #\n",
        "                # print (\"PIL_im = \" , Image.fromarray(np.uint8(img)))\n",
        "                # print (\"PIL_att = \" , Image.fromarray(np.uint8(one_map[:,:,:3])))\n",
        "                # print (\"img.size( :\" , img.shape)\n",
        "                # print (\"one_map.size( :\" , one_map.shape)\n",
        "                PIL_im = Image.fromarray(np.uint8(img))\n",
        "                PIL_att = Image.fromarray(np.uint8(one_map[:,:,:3]))\n",
        "                merged = \\\n",
        "                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
        "                #print (\"merged : \" , merged.size)\n",
        "                mask = Image.new('L', (vis_size, vis_size), (210))\n",
        "                #print (\" mask  : \" , mask.size)\n",
        "                merged.paste(PIL_im, (0, 0))\n",
        "                #print (\" merged.paste(PIL_im)  : \" , merged.size )\n",
        "                ############################################################\n",
        "                merged.paste(PIL_att, (0, 0), mask)\n",
        "                #print (\" merged.paste(PIL_att)  : \" ,  merged.size)#########################\n",
        "                merged = np.array(merged)[:, :, :3]\n",
        "                #print (\"  np.array(merged)[:::3] : \" , merged.size )#########################\n",
        "                ############################################################\n",
        "            else:\n",
        "                #print (\" IN THE ELSE post_pad : \" , post_pad.shape)\n",
        "                one_map = post_pad\n",
        "                #print (\" one_map  : \" , one_map.shape )\n",
        "                merged = post_pad\n",
        "                #print (\"  OUTTING THE ELSE : \" , merged.shape )\n",
        "            \n",
        "            #print (\"  row : \" , len(row))\n",
        "            row.append(one_map[:,:,:3])\n",
        "            #print (\"  row.appedn(one_map) : \" , len(row))\n",
        "            row.append(middle_pad)\n",
        "            #print (\"  row.append(middle_pad) : \" , len(row))\n",
        "            #\n",
        "            #print (\"  row_merge : \" , len(row_merge))\n",
        "            row_merge.append(merged)\n",
        "            #print (\"  row_merge.append(mereged) : \" , len(row_merge) )\n",
        "            row_merge.append(middle_pad)\n",
        "            #print (\"  row_merge.append(middle_pad) : \" , len(row_merge) )\n",
        "        ####################################################################\n",
        "        # print(\"row.shape : \", len(row))\n",
        "        # for i in range(len(row)):\n",
        "        #   print('arr', i,   \n",
        "        #         \" => dim0:\", len(row[i]),\n",
        "        #         \" || dim1:\", len(row[i][0]),\n",
        "        #         \" || dim2:\", len(row[i][0][0]))\n",
        "        # #print(row)\n",
        "        # print(\"row[0].shape : \", len(row[0]))\n",
        "        # #print(row[0])\n",
        "        # print(\"row[0][0].shape : \", len(row[0][0]))\n",
        "        # #print(row[0][0])\n",
        "        # print(\"row[0][0][0].shape : \", len(row[0][0][0]))\n",
        "        # #print(row[0][0][0])\n",
        "\n",
        "        # print(\"row[1].shape : \", len(row[1]))\n",
        "        # #print(row[1])\n",
        "        # print(\"row[1][0].shape : \", len(row[1][0]))\n",
        "        # #print(row[1][0])\n",
        "        # print(\"row[1][0][0].shape : \", len(row[1][0][0]))\n",
        "        # #print(row[1][0][0])\n",
        "\n",
        "        # print(\"row[2].shape : \", len(row[2]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[2][0].shape : \", len(row[2][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[2][0][0].shape : \", len(row[2][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[3].shape : \", len(row[3]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[3][0].shape : \", len(row[3][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[3][0][0].shape : \", len(row[3][0][0]))\n",
        "        # #print(row[2][0][0])\n",
        "\n",
        "        # print(\"row[4].shape : \", len(row[4]))\n",
        "        # #print(row[2])\n",
        "        # print(\"row[4][0].shape : \", len(row[4][0]))\n",
        "        # #print(row[2][0])\n",
        "        # print(\"row[4][0][0].shape : \", len(row[4][0][0]))\n",
        "        #print(row[2][0][0])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        row = np.concatenate(row, 1)\n",
        "        #print (\" row.conatent(1)  : \" ,  len(row))########################################\n",
        "        row_merge = np.concatenate(row_merge, 1)\n",
        "        #print (\"   : \" , )############################\n",
        "        ####################################################################\n",
        "        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
        "        if txt.shape[1] != row.shape[1]:\n",
        "            print('txt', txt.shape, 'row', row.shape)\n",
        "            bUpdate = 0\n",
        "            break\n",
        "        #####################################################################\n",
        "        row = np.concatenate([txt, row, row_merge], 0)#######################\n",
        "        img_set.append(row)##################################################\n",
        "        #####################################################################\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"bUpdate : \" , bUpdate)\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    if bUpdate:\n",
        "        img_set = np.concatenate(img_set, 0)\n",
        "        img_set = img_set.astype(np.uint8)\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return img_set, sentences\n",
        "    else:\n",
        "        print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "        print(\"build_super_images_start_time : \" , time.time() - build_super_images_start_time)\n",
        "        print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "        return None\n",
        "\n",
        "def conv1x1(in_planes, out_planes, bias=False):\n",
        "    \"1x1 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train',\n",
        "                    base_size=64,\n",
        "                    transform=None, target_transform=None, input_ids=None, segments_ids=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
        "\n",
        "        self.imsize = []# [299]\n",
        "        for i in range(cfg.TREE.BRANCH_NUM):\n",
        "            self.imsize.append(base_size)\n",
        "            base_size = base_size * 2\n",
        "        print(\"self.imsize\", self.imsize)\n",
        "\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        if data_dir.find('birds') != -1:\n",
        "            self.bbox = self.load_bbox() # 11788 long dictionry with key as image name and value is 4 ints list bounding box\n",
        "        else:\n",
        "            self.bbox = None\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames, self.captions, self.ixtoword, self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
        "        #filenames: List of 8855 text items of image names\n",
        "        #captions: List of 8855 varible lengths captions -in range 9-18 -\n",
        "        #ixtoword: dictionry  of 5450 index [key] to word [value] pairs\n",
        "        #wordtoix: dictionry  of 5450 word [key] to index [value] pairs\n",
        "        #n_words: 5450\n",
        "\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames)) #200 classes, len:8855\n",
        "\n",
        "        self.number_example = len(self.filenames) #8855\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.segments_ids = segments_ids\n",
        "\n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        #\n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        print('Total filenames: ', len(filenames), filenames[0])\n",
        "        #\n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        #\n",
        "        return filename_bbox\n",
        "\n",
        "    def load_captions(self, data_dir, filenames):\n",
        "        all_captions = []\n",
        "        for i in range(len(filenames)):\n",
        "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
        "            with open(cap_path, \"r\") as f:\n",
        "                captions = f.read().decode('utf8').split('\\n')\n",
        "                cnt = 0\n",
        "                for cap in captions:\n",
        "                    if len(cap) == 0:\n",
        "                        continue\n",
        "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    # picks out sequences of alphanumeric characters as tokens\n",
        "                    # and drops everything else\n",
        "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                    tokens = tokenizer.tokenize(cap.lower())\n",
        "                    # print('tokens', tokens)\n",
        "                    if len(tokens) == 0:\n",
        "                        print('cap', cap)\n",
        "                        continue\n",
        "\n",
        "                    tokens_new = []\n",
        "                    for t in tokens:\n",
        "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                        if len(t) > 0:\n",
        "                            tokens_new.append(t)\n",
        "                    all_captions.append(tokens_new)\n",
        "                    cnt += 1\n",
        "                    if cnt == self.embeddings_num:\n",
        "                        break\n",
        "                if cnt < self.embeddings_num:\n",
        "                    print('ERROR: the captions for %s less than %d'% (filenames[i], cnt))\n",
        "        return all_captions\n",
        "\n",
        "    def build_dictionary(self, train_captions, test_captions):\n",
        "        word_counts = defaultdict(float)\n",
        "        captions = train_captions + test_captions\n",
        "        for sent in captions:\n",
        "            for word in sent:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
        "\n",
        "        ixtoword = {}\n",
        "        ixtoword[0] = '<end>'\n",
        "        wordtoix = {}\n",
        "        wordtoix['<end>'] = 0\n",
        "        ix = 1\n",
        "        for w in vocab:\n",
        "            wordtoix[w] = ix\n",
        "            ixtoword[ix] = w\n",
        "            ix += 1\n",
        "\n",
        "        train_captions_new = []\n",
        "        for t in train_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            train_captions_new.append(rev)\n",
        "\n",
        "        test_captions_new = []\n",
        "        for t in test_captions:\n",
        "            rev = []\n",
        "            for w in t:\n",
        "                if w in wordtoix:\n",
        "                    rev.append(wordtoix[w])\n",
        "            # rev.append(0)  # do not need '<end>' token\n",
        "            test_captions_new.append(rev)\n",
        "\n",
        "        return [train_captions_new, test_captions_new,\n",
        "                ixtoword, wordtoix, len(ixtoword)]\n",
        "\n",
        "    def load_text_data(self, data_dir, split):\n",
        "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
        "        train_names = self.load_filenames(data_dir, 'train')\n",
        "        test_names = self.load_filenames(data_dir, 'test')\n",
        "        if not os.path.isfile(filepath):\n",
        "            train_captions = self.load_captions(data_dir, train_names)\n",
        "            test_captions = self.load_captions(data_dir, test_names)\n",
        "\n",
        "            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n",
        "                self.build_dictionary(train_captions, test_captions)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump([train_captions, test_captions,\n",
        "                                ixtoword, wordtoix], f, protocol=2)\n",
        "                print('Save to: ', filepath)\n",
        "        else:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                x = pickle.load(f)\n",
        "                train_captions, test_captions = x[0], x[1]\n",
        "                ixtoword, wordtoix = x[2], x[3]\n",
        "                del x\n",
        "                n_words = len(ixtoword)\n",
        "                print('Load from: ', filepath)\n",
        "        if split == 'train':\n",
        "            # a list of list: each list contains\n",
        "            # the indices of words in a sentence\n",
        "            captions = train_captions\n",
        "            filenames = train_names\n",
        "        else:  # split=='test'\n",
        "            captions = test_captions\n",
        "            filenames = test_names\n",
        "        return filenames, captions, ixtoword, wordtoix, n_words\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f , encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir, split):\n",
        "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                filenames = pickle.load(f)\n",
        "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        else:\n",
        "            filenames = []\n",
        "        return filenames\n",
        "\n",
        "    def get_caption(self, sent_ix):\n",
        "        # a list of indices for a sentence\n",
        "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
        "        if (sent_caption == 0).sum() > 0:\n",
        "            print('ERROR: do not need END (0) token', sent_caption)\n",
        "        num_words = len(sent_caption)\n",
        "        # pad with 0s (i.e., '<end>')\n",
        "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
        "        x_len = num_words\n",
        "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
        "            x[:num_words, 0] = sent_caption\n",
        "        else:\n",
        "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
        "            np.random.shuffle(ix)\n",
        "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
        "            ix = np.sort(ix)\n",
        "            x[:, 0] = sent_caption[ix]\n",
        "            x_len = cfg.TEXT.WORDS_NUM\n",
        "        return x, x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        key = self.filenames[index]\n",
        "        cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "        #\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        imgs = get_imgs(img_name, self.imsize,\n",
        "                        bbox, self.transform, normalize=self.norm)\n",
        "        # random select a sentence\n",
        "        sent_ix = random.randint(0, self.embeddings_num)\n",
        "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
        "        caps, cap_len = self.get_caption(new_sent_ix)\n",
        "        return imgs, caps, cap_len, cls_id, key, self.input_ids, self.segments_ids\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
        "                    nhidden=128, nlayers=1, bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = cfg.TEXT.WORDS_NUM # max length and padded to captions= 18\n",
        "        self.ntoken = ntoken  # size of the dictionary = 5450\n",
        "        self.ninput = ninput  # size of each embedding vector = 300\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed = 0.5\n",
        "        self.nlayers = nlayers  # Number of recurrent layers =1\n",
        "        self.bidirectional = bidirectional # True\n",
        "        self.rnn_type = cfg.RNN_TYPE #LSTM\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions # 128\n",
        "\n",
        "        self.define_module()\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
        "                                self.nlayers, batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
        "                                self.nlayers, batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable(weight.new(self.nlayers * self.num_directions, bsz, self.nhidden).zero_()),\n",
        "                    Variable(weight.new(self.nlayers * self.num_directions,bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
        "                                        bsz, self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        #emb[0]: a tensor of torch.Size([660, 300])\n",
        "        #emb[1]: a tensor of torch.Size([18])\n",
        "        #emb[2]: None\n",
        "        #emb[3]: None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "\n",
        "\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        #output[0]: a tensor of torch.Size([660, 256])\n",
        "        #output[1]: a tensor of torch.Size([18])\n",
        "        #output[2]: None\n",
        "        #output[3]: None\n",
        "        #hidden : a tuple of 2 tensors of torch.Size([2, 48, 128])\n",
        "\n",
        "\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0] # torch.Size([48, 18, 256])\n",
        "        \n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        \n",
        "        words_emb = output.transpose(1, 2) #torch.Size([48, 256, 18])\n",
        "        \n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()#torch.Size([48, 2, 128])\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)#torch.Size([48, 256])\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CNN_ENCODER(nn.Module):\n",
        "    def __init__(self, nef):\n",
        "        super(CNN_ENCODER, self).__init__()\n",
        "        if cfg.TRAIN.FLAG:\n",
        "            self.nef = nef\n",
        "        else:\n",
        "            self.nef = 256  # define a uniform ranker\n",
        "\n",
        "        model = models.inception_v3()\n",
        "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
        "        model.load_state_dict(model_zoo.load_url(url))\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from ', url)\n",
        "        # print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
        "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
        "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
        "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
        "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
        "        self.Mixed_5b = model.Mixed_5b\n",
        "        self.Mixed_5c = model.Mixed_5c\n",
        "        self.Mixed_5d = model.Mixed_5d\n",
        "        self.Mixed_6a = model.Mixed_6a\n",
        "        self.Mixed_6b = model.Mixed_6b\n",
        "        self.Mixed_6c = model.Mixed_6c\n",
        "        self.Mixed_6d = model.Mixed_6d\n",
        "        self.Mixed_6e = model.Mixed_6e\n",
        "        self.Mixed_7a = model.Mixed_7a\n",
        "        self.Mixed_7b = model.Mixed_7b\n",
        "        self.Mixed_7c = model.Mixed_7c\n",
        "\n",
        "        self.emb_features = conv1x1(768, self.nef)\n",
        "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
        "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = None\n",
        "        # --> fixed-size input: batch x 3 x 299 x 299\n",
        "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "\n",
        "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        # image region features\n",
        "        features = x\n",
        "        # 17 x 17 x 768\n",
        "\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.Mixed_7c(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = F.avg_pool2d(x, kernel_size=8)\n",
        "        # 1 x 1 x 2048\n",
        "        # x = F.dropout(x, training=self.training)\n",
        "        # 1 x 1 x 2048\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 2048\n",
        "\n",
        "        # global image features\n",
        "        cnn_code = self.emb_cnn_code(x)\n",
        "        # 512\n",
        "        if features is not None:\n",
        "            features = self.emb_features(features)\n",
        "        return features, cnn_code\n",
        "\n",
        "class BERT_ENCODER(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT_ENCODER, self).__init__()\n",
        "\n",
        "        \n",
        "        model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from  BertModel')\n",
        "        print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "        self.bert_model = model\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.word_bert_code = nn.Linear(768, 256)\n",
        "        self.sent_bert_code = nn.Linear(768, 256)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.word_bert_code.weight.data.uniform_(-initrange, initrange)\n",
        "        self.sent_bert_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self,  b_input_ids, b_segments_ids):\n",
        "        \n",
        "        outputs = self.bert_model(b_input_ids, b_segments_ids)\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        a = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "\n",
        "        word_embedding= self.word_bert_code(a)\n",
        "\n",
        "\n",
        "        word_embedding = word_embedding.permute(1,3,0,2)\n",
        "\n",
        "        word_embedding = word_embedding.sum(dim=2)\n",
        "\n",
        "\n",
        "        print ('Word Embedding Shape is: ' , word_embedding.size())\n",
        "\n",
        "        token_vecs = hidden_states[-2]\n",
        "\n",
        "        # Calculate the average of all 64 token vectors.\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=1)\n",
        "        sentence_embedding= self.sent_bert_code(sentence_embedding)\n",
        "        print('Sentence Embedding Shape is: ', sentence_embedding.size())\n",
        "\n",
        "\n",
        "        return  word_embedding, sentence_embedding\n",
        "\n",
        "\n",
        "\n",
        "def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
        "    \n",
        "    FONT_MAX = 50\n",
        "\n",
        "    num = captions.size(0)\n",
        "    img_txt = Image.fromarray(convas)\n",
        "    # get a font\n",
        "    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    print (\"CURRENT WORKING DIRCTORY : \" , os.getcwd())\n",
        "    fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
        "    # get a drawing context\n",
        "    d = ImageDraw.Draw(img_txt)\n",
        "    sentence_list = []\n",
        "    for i in range(num):\n",
        "        cap = captions[i].data.cpu().numpy()\n",
        "        sentence = []\n",
        "        for j in range(len(cap)):\n",
        "            if cap[j] == 0:\n",
        "                break\n",
        "            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
        "            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
        "                   font=fnt, fill=(255, 255, 255, 255))\n",
        "            sentence.append(word)\n",
        "        sentence_list.append(sentence)\n",
        "    return img_txt, sentence_list\n",
        "\n",
        "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
        "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
        "    \"\"\"\n",
        "    w12 = torch.sum(x1 * x2, dim)\n",
        "    w1 = torch.norm(x1, 2, dim)\n",
        "    w2 = torch.norm(x2, 2, dim)\n",
        "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
        "\n",
        "def sent_loss(cnn_code, rnn_code, labels, class_ids,\n",
        "              batch_size, eps=1e-8):\n",
        "    # ### Mask mis-match samples  ###\n",
        "    # that come from the same class as the real sample ###\n",
        "    masks = []\n",
        "    if class_ids is not None:\n",
        "        for i in range(batch_size):\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    # --> seq_len x batch_size x nef\n",
        "    if cnn_code.dim() == 2:\n",
        "        cnn_code = cnn_code.unsqueeze(0)\n",
        "        rnn_code = rnn_code.unsqueeze(0)\n",
        "\n",
        "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
        "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
        "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
        "    # scores* / norm*: seq_len x batch_size x batch_size\n",
        "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
        "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
        "    scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "\n",
        "    # --> batch_size x batch_size\n",
        "    scores0 = scores0.squeeze()\n",
        "    if class_ids is not None:\n",
        "        scores0.data.masked_fill_(masks, -float('inf'))\n",
        "    scores1 = scores0.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1\n",
        "\n",
        "\n",
        "def words_loss(img_features, words_emb, labels, cap_lens, class_ids, batch_size):\n",
        "    \"\"\"\n",
        "        words_emb(query): batch x nef x seq_len\n",
        "        img_features(context): batch x nef x 17 x 17\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    att_maps = []\n",
        "    similarities = []\n",
        "    cap_lens = cap_lens.data.tolist()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "    \n",
        "        if class_ids is not None:\n",
        "            mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
        "            mask[i] = 0\n",
        "            masks.append(mask.reshape((1, -1)))\n",
        "        # Get the i-th text description\n",
        "        words_num = cap_lens[i]\n",
        "        # -> 1 x nef x words_num\n",
        "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
        "        # -> batch_size x nef x words_num\n",
        "        word = word.repeat(batch_size, 1, 1)\n",
        "        # batch x nef x 17*17\n",
        "        context = img_features\n",
        "        \"\"\"\n",
        "            word(query): batch x nef x words_num\n",
        "            context: batch x nef x 17 x 17\n",
        "            weiContext: batch x nef x words_num\n",
        "            attn: batch x words_num x 17 x 17\n",
        "        \"\"\"\n",
        "        weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
        "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
        "        # --> batch_size x words_num x nef\n",
        "        word = word.transpose(1, 2).contiguous()\n",
        "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
        "        # --> batch_size*words_num x nef\n",
        "        word = word.view(batch_size * words_num, -1)\n",
        "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
        "        #\n",
        "        # -->batch_size*words_num\n",
        "        row_sim = cosine_similarity(word, weiContext)\n",
        "        # --> batch_size x words_num\n",
        "        row_sim = row_sim.view(batch_size, words_num)\n",
        "\n",
        "        # Eq. (10)\n",
        "        row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
        "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
        "        row_sim = torch.log(row_sim)\n",
        "\n",
        "        # --> 1 x batch_size\n",
        "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
        "        similarities.append(row_sim)\n",
        "\n",
        "\n",
        "    # batch_size x batch_size\n",
        "    similarities = torch.cat(similarities, 1)\n",
        "    if class_ids is not None:\n",
        "        masks = np.concatenate(masks, 0)\n",
        "        # masks: batch_size x batch_size\n",
        "        masks = torch.BoolTensor(masks)\n",
        "        if cfg.CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
        "    if class_ids is not None:\n",
        "        similarities.data.masked_fill_(masks, -float('inf'))\n",
        "    similarities1 = similarities.transpose(0, 1)\n",
        "    if labels is not None:\n",
        "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
        "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
        "    else:\n",
        "        loss0, loss1 = None, None\n",
        "    return loss0, loss1, att_maps\n",
        "\n",
        "def func_attention(query, context, gamma1):\n",
        "    \"\"\"\n",
        "    query: batch x ndf x queryL\n",
        "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
        "    mask: batch_size x sourceL\n",
        "    \"\"\"\n",
        "    batch_size, queryL = query.size(0), query.size(2)\n",
        "    ih, iw = context.size(2), context.size(3)\n",
        "    sourceL = ih * iw\n",
        "\n",
        "    # --> batch x sourceL x ndf\n",
        "    context = context.view(batch_size, -1, sourceL)\n",
        "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
        "\n",
        "    # Get attention\n",
        "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
        "    # -->batch x sourceL x queryL\n",
        "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
        "    # --> batch*sourceL x queryL\n",
        "    attn = attn.view(batch_size*sourceL, queryL)\n",
        "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
        "\n",
        "    # --> batch x sourceL x queryL\n",
        "    attn = attn.view(batch_size, sourceL, queryL)\n",
        "    # --> batch*queryL x sourceL\n",
        "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "    attn = attn.view(batch_size*queryL, sourceL)\n",
        "    #  Eq. (9)\n",
        "    attn = attn * gamma1\n",
        "    attn = nn.Softmax()(attn)\n",
        "    attn = attn.view(batch_size, queryL, sourceL)\n",
        "    # --> batch x sourceL x queryL\n",
        "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
        "    # --> batch x ndf x queryL\n",
        "    weightedContext = torch.bmm(context, attnT)\n",
        "\n",
        "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "\n",
        "def train(dataloader, cnn_model, rnn_model, batch_size, labels, optimizer, epoch, ixtoword, image_dir, bert_encoder):\n",
        "    train_function_start_time = time.time()\n",
        "    cnn_model.train() #Sets the module in training mode.\n",
        "    #rnn_model.train() #Sets the module in training mode.\n",
        "    \n",
        "    bert_encoder.cuda()\n",
        "    bert_encoder.eval()\n",
        "    \n",
        "    s_total_loss0 = 0\n",
        "    s_total_loss1 = 0\n",
        "    w_total_loss0 = 0\n",
        "    w_total_loss1 = 0\n",
        "    \n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    # print(\"len(dataloader) : \" , len(dataloader) )\n",
        "    # print(\" count = \" ,  (epoch + 1) * len(dataloader)  )\n",
        "    # print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "    count = (epoch + 1) * len(dataloader)\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "    \n",
        "    \n",
        "        # Loading the first batch (number of batches/steps in an epoch is 183)\n",
        "        #rnn_model.zero_grad()\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        imgs, captions, cap_lens, class_ids, keys, b_input_ids, b_segments_ids = prepare_data(data)\n",
        "\n",
        "\n",
        "        # words_features: batch_size x 256 x 17 x 17 ==> # image region features\n",
        "        # sent_code: batch_size x 256                ==> # global image features\n",
        "        words_features, sent_code = cnn_model(imgs[-1])\n",
        "        # --> batch_size x nef x 17*17\n",
        "        nef, att_sze = words_features.size(1), words_features.size(2)# 256, 17(16th of the whole image)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        #hidden = rnn_model.init_hidden(batch_size) # A tuple of 2 zero tensor of torch.Size([2, 48, 128])\n",
        "        # words_emb: batch_size x nef x seq_len\n",
        "        # sent_emb: batch_size x nef\n",
        "        #words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        #-------------------------------------------------------------------------------\n",
        "                #---------------------------------------------------------\n",
        "                        #-----------------------------------\n",
        "        words_emb, sent_emb =bert_encoder(b_input_ids, b_segments_ids)\n",
        "                        #-----------------------------------\n",
        "                #---------------------------------------------------------\n",
        "        #-------------------------------------------------------------------------------\n",
        "\n",
        "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels, cap_lens, class_ids, batch_size)\n",
        "        w_total_loss0 += w_loss0.data\n",
        "        w_total_loss1 += w_loss1.data\n",
        "        loss = w_loss0 + w_loss1\n",
        "\n",
        "        s_loss0, s_loss1 = sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        loss += s_loss0 + s_loss1\n",
        "        s_total_loss0 += s_loss0.data\n",
        "        s_total_loss1 += s_loss1.data\n",
        "        #\n",
        "        loss.backward()\n",
        "        #\n",
        "        # `clip_grad_norm` helps prevent\n",
        "        # the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(), cfg.TRAIN.RNN_GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % UPDATE_INTERVAL == 0:\n",
        "            count = epoch * len(dataloader) + step\n",
        "\n",
        "            # print (\"====================================================\")\n",
        "            # print (\"s_total_loss0 : \" , s_total_loss0)\n",
        "            # print (\"s_total_loss0.item() : \" , s_total_loss0.item())\n",
        "            # print (\"UPDATE_INTERVAL : \" , UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss0.item()/UPDATE_INTERVAL : \" , s_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"s_total_loss1.item()/UPDATE_INTERVAL : \" , s_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss0.item()/UPDATE_INTERVAL : \" , w_total_loss0.item()/UPDATE_INTERVAL)\n",
        "            print (\"w_total_loss1.item()/UPDATE_INTERVAL : \" , w_total_loss1.item()/UPDATE_INTERVAL)\n",
        "            # print (\"s_total_loss0/UPDATE_INTERVAL : \" , s_total_loss0/UPDATE_INTERVAL)\n",
        "            # print (\"=====================================================\")\n",
        "            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n",
        "            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n",
        "            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    's_loss {:5.2f} {:5.2f} | '\n",
        "                    'w_loss {:5.2f} {:5.2f}'\n",
        "                    .format(epoch, step, len(dataloader),\n",
        "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
        "                            s_cur_loss0, s_cur_loss1,\n",
        "                            w_cur_loss0, w_cur_loss1))\n",
        "            s_total_loss0 = 0\n",
        "            s_total_loss1 = 0\n",
        "            w_total_loss0 = 0\n",
        "            w_total_loss1 = 0\n",
        "            start_time = time.time()\n",
        "            # attention Maps\n",
        "            #Save image only every 8 epochs && Save it to The Drive\n",
        "            if (epoch % 8 == 0):\n",
        "                print(\"bulding images\")\n",
        "                img_set, _ = \\\n",
        "                    build_super_images(imgs[-1].cpu(), captions,\n",
        "                                    ixtoword, attn_maps, att_sze)\n",
        "                if img_set is not None:\n",
        "                    im = Image.fromarray(img_set)\n",
        "                    fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
        "                    im.save(fullpath)\n",
        "                    mydriveimg = '/content/drive/My Drive/cubImage'\n",
        "                    drivepath = '%s/attention_maps%d.png' % (mydriveimg, epoch)\n",
        "                    im.save(drivepath)\n",
        "    print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "    print(\"train_function_time : \" , time.time() - train_function_start_time)\n",
        "    print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "    return count\n",
        "\n",
        "\n",
        "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
        "    cnn_model.eval()\n",
        "    rnn_model.eval()\n",
        "    s_total_loss = 0\n",
        "    w_total_loss = 0\n",
        "    for step, data in enumerate(dataloader, 0):\n",
        "        real_imgs, captions, cap_lens, class_ids, keys, input_ids, segments_ids = prepare_data(data)\n",
        "\n",
        "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
        "        # nef = words_features.size(1)\n",
        "        # words_features = words_features.view(batch_size, nef, -1)\n",
        "\n",
        "        hidden = rnn_model.init_hidden(batch_size)\n",
        "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
        "\n",
        "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
        "                                            cap_lens, class_ids, batch_size)\n",
        "        w_total_loss += (w_loss0 + w_loss1).data\n",
        "\n",
        "        s_loss0, s_loss1 = \\\n",
        "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
        "        s_total_loss += (s_loss0 + s_loss1).data\n",
        "\n",
        "        if step == 50:\n",
        "            break\n",
        "\n",
        "    s_cur_loss = s_total_loss.item() / step\n",
        "    w_cur_loss = w_total_loss.item() / step\n",
        "\n",
        "    return s_cur_loss, w_cur_loss\n",
        "\n",
        "\n",
        "def build_models():\n",
        "    # build model ############################################################\n",
        "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
        "    '''\n",
        "    RNN_ENCODER(\n",
        "    (encoder): Embedding(5450, 300)\n",
        "    (drop): Dropout(p=0.5, inplace=False)\n",
        "    (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True))\n",
        "    '''\n",
        "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
        "    bert_encoder = BERT_ENCODER()\n",
        "\n",
        "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
        "    '''\n",
        "    A tensor of [0,1,2,3,...,47]\n",
        "    '''\n",
        "    start_epoch = 0\n",
        "    if cfg.TRAIN.NET_E != '':\n",
        "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
        "        text_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', cfg.TRAIN.NET_E)\n",
        "        #\n",
        "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
        "        state_dict = torch.load(name)\n",
        "        image_encoder.load_state_dict(state_dict)\n",
        "        print('Load ', name)\n",
        "\n",
        "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
        "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
        "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
        "        start_epoch = int(start_epoch) + 1\n",
        "        print('start_epoch', start_epoch)\n",
        "    if cfg.CUDA:\n",
        "        text_encoder = text_encoder.cuda()\n",
        "        image_encoder = image_encoder.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return text_encoder, image_encoder, labels, start_epoch, bert_encoder\n",
        "\n",
        "\n",
        "__name__ = \"__main__\"\n",
        "if __name__ == \"__main__\":\n",
        "    print('Using config:')\n",
        "    pprint.pprint(cfg)\n",
        "\n",
        "    UPDATE_INTERVAL = 200\n",
        "\n",
        "    ##########################################################################\n",
        "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
        "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
        "    output_dir = '../output/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
        "\n",
        "    model_dir = os.path.join(output_dir, 'Model')\n",
        "    image_dir = os.path.join(output_dir, 'Image')\n",
        "    mkdir_p(model_dir)\n",
        "    mkdir_p(image_dir)\n",
        "\n",
        "    torch.cuda.set_device(cfg.GPU_ID)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Get data loader ##################################################\n",
        "    imsize = 299\n",
        "    batch_size = 48\n",
        "\n",
        "    image_transform = transforms.Compose([transforms.Scale(355), transforms.RandomCrop(imsize), transforms.RandomHorizontalFlip()])\n",
        "    \n",
        "    dataset = TextDataset(cfg.DATA_DIR, 'train', base_size=cfg.TREE.BASE_SIZE, transform=image_transform, input_ids=input_ids, segments_ids=segments_ids)\n",
        "    print(dataset.n_words, dataset.embeddings_num)\n",
        "    assert dataset\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "    #using prepare data functiont this dataloader yieldes:\n",
        "    #imgs: a list of 1 tensor of size torch.Size([48, 3, 299, 299])\n",
        "    #captons: a  tensor of size torch.Size([48, 18]), shorter filled with end words converted by word to index\n",
        "    #cap_lens: a  tensor of size torch.Size([48]) , acual caps lens order from big to small (max is 18)\n",
        "    #class_ids: a 48 ints list in range 0-200 of the classes\n",
        "    #keys: a 48 string list  of the classes classes nammes crosspondening to the class_ids\n",
        "    \n",
        "\n",
        "    # # validation data #\n",
        "    dataset_val = TextDataset(cfg.DATA_DIR, 'test', base_size=cfg.TREE.BASE_SIZE,transform=image_transform)\n",
        "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, drop_last=True,shuffle=True, num_workers=int(cfg.WORKERS))\n",
        "\n",
        "    # Train ##############################################################\n",
        "    text_encoder, image_encoder, labels, start_epoch, bert_encoder = build_models()\n",
        "    para = list(text_encoder.parameters()) # 9 paramters\n",
        "    for v in image_encoder.parameters(): # 3 parameters\n",
        "        if v.requires_grad:\n",
        "            para.append(v)\n",
        "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "\n",
        "    try:\n",
        "        lr = cfg.TRAIN.ENCODER_LR #0.002\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "        print(\"Start_epoch : \" , start_epoch)\n",
        "        print(\"cfg.TRAIN.MAX_EPOCH : \" , cfg.TRAIN.MAX_EPOCH )\n",
        "        print(\"keyword |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "\n",
        "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
        "            \n",
        "            one_epoch_start_time = time.time()\n",
        "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
        "            epoch_start_time = time.time()\n",
        "            count = train(dataloader, image_encoder, text_encoder, batch_size, labels, optimizer, epoch, dataset.ixtoword, image_dir, bert_encoder)\n",
        "            print('-' * 89)\n",
        "            if len(dataloader_val) > 0:\n",
        "                s_loss, w_loss = evaluate(dataloader_val, image_encoder, text_encoder, batch_size)\n",
        "                print('| end epoch {:3d} | valid loss ''{:5.2f} {:5.2f} | lr {:.5f}|'.format(epoch, s_loss, w_loss, lr))\n",
        "            print('-' * 89)\n",
        "            if lr > 0.0002 : #cfg.TRAIN.ENCODER_LR/10.:\n",
        "                lr *= 0.98\n",
        "\n",
        "            print(\"keyTime |||||||||||||||||||||||||||||||\")\n",
        "            print(\"one_epoch_time : \" , time.time() - one_epoch_start_time)\n",
        "            print(\"KeyTime |||||||||||||||||||||||||||||||\")\n",
        "\n",
        "            if (epoch % 8 == 0 or epoch == cfg.TRAIN.MAX_EPOCH or epoch == cfg.TRAIN.MAX_EPOCH-1 ):\n",
        "                mydrivemodel = '/content/drive/My Drive/cubModel'\n",
        "                torch.save(image_encoder.state_dict(), '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(image_encoder.state_dict(), '%s/image_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                torch.save(text_encoder.state_dict(), '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
        "                torch.save(text_encoder.state_dict(), '%s/text_encoder%d.pth' % (mydrivemodel, epoch))\n",
        "                print('Save G/Ds models.')\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V1SmOR4s8vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQc7zyFFs9NA",
        "colab_type": "text"
      },
      "source": [
        "# ==================================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCAHpdG8s8Ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5TXyhpHs067",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fc7aeac8-1b72-4822-9881-ad572797e8a1"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzf9wU9bs6HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9eA0WpVtKyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4a4e3f4e-7ad1-4d8c-b07b-3c5702f4edfe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/CUB_captions.csv\", header=None, names=['Label',  'ID', 'Sentences'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,856\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A1QKcPcuVFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "import numpy as np\n",
        "\n",
        "sentences = df.Sentences.values\n",
        "labels_text = df.Label.values\n",
        "labels = df.ID.values\n",
        "\n",
        "labels = np.delete(labels, 0)\n",
        "labels_text = np.delete(labels_text, 0)\n",
        "sentences = np.delete(sentences, 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgjkL91muXZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "190327803dba4965905cf88adedf3283",
            "7d1c2361e7c74b80b264f7835ebad887",
            "9909724ec9be42088cebf774a5c4b563",
            "f2b5e3eca82e4394b6600528eeda07dd",
            "430f05d8c8994d248b503d251901fee6",
            "7a49b59cec5944af9e8a6274ca1a78d0",
            "de9771f9d52340e9be4f4ac21f32310e",
            "09c8944ff1cf44a1958c31c1f5dc4e5a"
          ]
        },
        "outputId": "4faeb3d9-a0ab-4eed-dd15-3f8f23dda992"
      },
      "source": [
        "from transformers import BertTokenizer , BertModel\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "190327803dba4965905cf88adedf3283",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxNNh29fuY46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "949ee733-44ab-4537-d2df-3bbdff813c05"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    if  len(input_ids) == 80:\n",
        "      print('sent with max len :', sent)\n",
        "      print('number of words insent with max len :', len(sent.split()))\n",
        "      print('index of sent with max len :', np.where(sentences==sent))\n",
        "\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "atwal = []\n",
        "for i in sentences:\n",
        "  atwal.append(len(i.split()))\n",
        "atwal.sort(reverse=True)\n",
        "print('Max number of words in a sentnces length: ', atwal[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  70\n",
            "Max number of words in a sentnces length:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpFTH4nLupkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "36dd75ac-fd3f-4542-c49c-690c3f6cd7f1"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 18,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation=True\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = labels.astype(int)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "segments_ids = []\n",
        "for i in range(input_ids.size()[0]):\n",
        "  segments_id = [1] * input_ids.size()[1]\n",
        "  segments_ids.append(segments_id)\n",
        "\n",
        "segments_ids = torch.tensor(segments_ids)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  a bird with a very long wing span and a long pointed beak.\n",
            "Token IDs: tensor([  101,  1037,  4743,  2007,  1037,  2200,  2146,  3358,  8487,  1998,\n",
            "         1037,  2146,  4197, 23525,  1012,   102,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEYcATczyenf",
        "colab_type": "text"
      },
      "source": [
        "#stop hina"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owlo9tjRutPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "254cf71a-8959-41cb-86f0-aec9b102945d"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, segments_ids, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(1 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset = dataset\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8,855 training samples\n",
            "    0 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9iURgn0uvp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 48\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsmXQV037kZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bjqoEGxEEq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ecb79c3b577c47df84fe30ef12adda64",
            "b5d66bf5423d4e93944c882911c9ec58",
            "93d46d39c387475a923ec0cbf03966ed",
            "2f4809e1004d41f3857a6ad21d72b1a5",
            "544aff2f8f124a45bd243d5ff7cb8a5a",
            "eb3fa6c92e8a480b8410c05f899a6a92",
            "1f74aeb478b44620a696682ed586a73f",
            "3708fbc2f9f049e28065df3d0f72d65c",
            "af956aac7fca4e53bbf0d0a39ef8873d",
            "79d6e3a33910469dbaa92ce658b5e46b",
            "b158fa5e20ed4c42a6a461b53fcf9c93",
            "0c322d22899741e5ab65d010d32bac3f",
            "5dae5867589842deb825449082e6a237",
            "d213177d12bc4f7ca79a3aef4d9628a5",
            "0f3fe60e2b6045a38ad17b28de9b972e",
            "47eaa5db3c1642f58d08f5dcd3a1963c"
          ]
        },
        "outputId": "0bcb8567-8ce0-42f2-d660-c022a35cb09e"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "bert_model.cuda()\n",
        "bert_model.eval()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecb79c3b577c47df84fe30ef12adda64",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af956aac7fca4e53bbf0d0a39ef8873d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMY8UgAI7ne8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e4df0c95-4d90-4188-881a-94f932b98fd4"
      },
      "source": [
        "for step, batch in enumerate(train_dataloader):\n",
        "  if step % 40 == 0 :  \n",
        "    # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "    # `to` method.\n",
        "    #\n",
        "    # `batch` contains three pytorch tensors:\n",
        "    #   [0]: input ids \n",
        "    #   [1]: attention masks\n",
        "    #   [2]: labels \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_segments_ids = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    print(device,'=> ', step)\n",
        "    #print(b_segments_ids.size())\n",
        "    print(b_input_ids.size())\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "          outputs = bert_model(b_input_ids, b_segments_ids)\n",
        "          hidden_states = outputs[2]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda =>  0\n",
            "torch.Size([48, 18])\n",
            "cuda =>  40\n",
            "torch.Size([48, 18])\n",
            "cuda =>  80\n",
            "torch.Size([48, 18])\n",
            "cuda =>  120\n",
            "torch.Size([48, 18])\n",
            "cuda =>  160\n",
            "torch.Size([48, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlICg2E78caz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.stack(hidden_states, dim=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofj7QKjp8qWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3417b1d0-1653-4102-d185-d1cb97a970b6"
      },
      "source": [
        "a.size()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 48, 18, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFyZYQZIEQ9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ef47d1cb-43da-4939-8cf7-3891767287e0"
      },
      "source": [
        "emb_cnn_code"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=256, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4ysPGLEEVDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = emb_cnn_code(a)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7B4xYE8EkG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1727b579-c277-478a-ff2e-ce13cd45912a"
      },
      "source": [
        "c.size()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 48, 18, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXUkoD8AGIK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " b = a.permute(1,3,0,2)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbmgmyH2GLKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5a0c2bae-e8f5-4f48-f0be-90a0fec59ff1"
      },
      "source": [
        "b.size()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 768, 13, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDDk7DiRGx1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = torch.ones_like(b)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GibwgYQaHOqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17fgJ9tPHVs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = f.sum(dim=2)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Pkezx1HjQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "55150da8-701a-4e84-97cb-447bd9991ac6"
      },
      "source": [
        "g[0].size()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J21jJ6BoHfTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f6f0d621-897e-41f6-d419-faca927627c9"
      },
      "source": [
        "g.size()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 768, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QVHLsaLNl6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3475b6a9-8e4c-4f62-e809-77862946a4f8"
      },
      "source": [
        " b = a.permute(1,2,0,3)\n",
        "\n",
        "word_vecs_sum = []\n",
        "for sentence in b:\n",
        "  # `sentence` is a [64 x 12 x 768] tensor.\n",
        "  \n",
        "  # Stores the token vectors, with shape [64 x 768]\n",
        "  token_vecs_sum =  []\n",
        "  # For each token in the sentence...\n",
        "  \n",
        "  for token in sentence:\n",
        "    # `token` is a [12 x 768] tensor\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "  # Use `token_vecs_sum` to represent `sentence`.\n",
        "  word_vecs_sum.append(token_vecs_sum)\n",
        "\n",
        "print ('Word Embedding Shape is: %d x %d x %d' % (len(word_vecs_sum), len(word_vecs_sum[0]), len(word_vecs_sum[0][0])))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Embedding Shape is: 48 x 18 x 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnh8rdSNNyJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "62657abc-54b1-4f99-de0e-87a6e4d22716"
      },
      "source": [
        "token_vecs = hidden_states[-2]\n",
        "\n",
        "# Calculate the average of all 64 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=1)\n",
        "print('Sentence Embedding Shape is: ', sentence_embedding.size())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Embedding Shape is:  torch.Size([48, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LUTMrtdPN1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_cnn_code = torch.nn.Linear(768, 256)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AUmF0HU_qah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3d378b00-4b9c-4eed-c08e-2fa6257c8d4f"
      },
      "source": [
        "emb_cnn_code.cuda()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=256, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t45uwKiy-rSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e= emb_cnn_code(sentence_embedding)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGLTizaa_zwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "028f393c-907b-41bb-e867-7763b5b956c3"
      },
      "source": [
        "e.size()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTeUi-Vn_Zx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzZBuQ8zCBwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9LJcyVbCBst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7LvYgT2CBqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq6gtBEzCBMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERT_ENCODER(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT_ENCODER, self).__init__()\n",
        "\n",
        "        \n",
        "        model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Load pretrained model from  BertModel')\n",
        "        print(model)\n",
        "\n",
        "        self.define_module(model)\n",
        "        self.init_trainable_weights()\n",
        "\n",
        "        self.bert_model = model\n",
        "\n",
        "    def define_module(self, model):\n",
        "        self.word_bert_code = nn.Linear(768, 256)\n",
        "        self.sent_bert_code = nn.Linear(768, 256)\n",
        "\n",
        "    def init_trainable_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.word_bert_code.weight.data.uniform_(-initrange, initrange)\n",
        "        self.sent_bert_code.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self,  b_input_ids, b_segments_ids):\n",
        "        \n",
        "        outputs = self.bert_model(b_input_ids, b_segments_ids)\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        a = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "\n",
        "        word_embedding= self.word_bert_code(a)\n",
        "\n",
        "\n",
        "        word_embedding = word_embedding.permute(1,3,0,2)\n",
        "\n",
        "        word_embedding = word_embedding.sum(dim=2)\n",
        "\n",
        "\n",
        "        print ('Word Embedding Shape is: ' , word_embedding.size())\n",
        "\n",
        "        token_vecs = hidden_states[-2]\n",
        "\n",
        "        # Calculate the average of all 64 token vectors.\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=1)\n",
        "        sentence_embedding= self.sent_bert_code(sentence_embedding)\n",
        "        print('Sentence Embedding Shape is: ', sentence_embedding.size())\n",
        "\n",
        "\n",
        "        return  word_embedding, sentence_embedding"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBajkKGLCCpS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7af0f9a-bab8-437a-b867-64fcf4bf76d8"
      },
      "source": [
        "bert_encoder = BERT_ENCODER()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load pretrained model from  BertModel\n",
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWuLRwPRCT3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b573022f-63d8-49b2-f465-bea4c95ba06e"
      },
      "source": [
        "bert_encoder.cuda()\n",
        "bert_encoder.eval()    "
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_ENCODER(\n",
              "  (word_bert_code): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (sent_bert_code): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (bert_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6EbD5HECp55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c9fb6fd-5e73-44d1-a7d5-0b84043db16e"
      },
      "source": [
        "w,s = bert_encoder(b_input_ids, b_segments_ids)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Embedding Shape is:  torch.Size([48, 256, 18])\n",
            "Sentence Embedding Shape is:  torch.Size([48, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPMCiXBVKOgq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9ab3c172-da21-4452-ad01-32cab4880daa"
      },
      "source": [
        "s.size()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2BcgzD5Cx-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b5766f0-424d-4eed-8a21-0f22748aa703"
      },
      "source": [
        "for param in bert_encoder.parameters():\n",
        "  print(param.requires_grad)\n",
        "  if param.requires_grad:\n",
        "    print(param.size())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "torch.Size([256, 768])\n",
            "True\n",
            "torch.Size([256])\n",
            "True\n",
            "torch.Size([256, 768])\n",
            "True\n",
            "torch.Size([256])\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YzRi4EbDa0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}